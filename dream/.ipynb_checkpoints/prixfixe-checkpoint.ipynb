{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6756468f-6f22-45e5-b2e5-af6691ab1518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muntakimrafi/anaconda3/envs/dream/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from prixfixe.autosome import (AutosomeCoreBlock,\n",
    "                      AutosomeFirstLayersBlock,\n",
    "                      AutosomeFinalLayersBlock,\n",
    "                      AutosomeTrainer,\n",
    "                      AutosomeDataProcessor)\n",
    "\n",
    "from prixfixe.bhi import (BHICoreBlock,\n",
    "                      BHIFirstLayersBlock,\n",
    "                      BHIFinalLayersBlock,\n",
    "                      BHITrainer,\n",
    "                      BHIDataProcessor)\n",
    "\n",
    "from prixfixe.unlockdna import (\n",
    "                      UnlockDNAFirstLayersBlock,\n",
    "                      UnlockDNACoreBlock,\n",
    "                      UnlockDNAFinalLayersBlock,\n",
    "                      UnlockDNATrainer,\n",
    "                      UnlockDNADataProcessor\n",
    ")\n",
    "\n",
    "from prixfixe.prixfixe import PrixFixeNet\n",
    "from prixfixe.prixfixe import CoreBlock\n",
    "from typing import List\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103974e2-ec8b-40b7-971c-ebaa6bc679e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdb5b0727b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = \"data/demo_train.txt\"\n",
    "VALID_DATA_PATH = \"data/demo_val.txt\"\n",
    "\n",
    "# TRAIN_DATA_PATH = \"/home/rafi11/projects/rrg-cdeboer/rafi11/DREAMNet/paper_runs_2_pos/data/train_0.txt\"\n",
    "# VALID_DATA_PATH = \"/home/rafi11/projects/rrg-cdeboer/rafi11/DREAMNet/paper_runs_2_pos/data/val_0.txt\"\n",
    "\n",
    "TRAIN_BATCH_SIZES = [512, 512, 256]\n",
    "N_PROCS = 8\n",
    "VALID_BATCH_SIZES = [512, 512, 256]\n",
    "PLASMID_PATH = \"plasmid.json\"\n",
    "SEQ_SIZES = [150, 110, 200]\n",
    "HEAD_LEN = 17\n",
    "TAIL_LEN = 13\n",
    "N_PROCS = 8\n",
    "\n",
    "with open(f'{TRAIN_DATA_PATH}', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "BATCH_PER_EPOCHS = [len(lines) // TRAIN_BATCH_SIZES[0], len(lines) // TRAIN_BATCH_SIZES[1], len(lines) // TRAIN_BATCH_SIZES[2]]\n",
    "with open(f'{VALID_DATA_PATH}', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "BATCH_PER_VALIDATIONS = [len(lines) // VALID_BATCH_SIZES[0], len(lines) // VALID_BATCH_SIZES[1], len(lines) // VALID_BATCH_SIZES[2]]\n",
    "\n",
    "device = torch.device(f\"cuda:0\")\n",
    "embedding_dim = 256\n",
    "n_blocks = 4\n",
    "kmer = 10\n",
    "input_dim = 6\n",
    "strides = 1\n",
    "ratio = 0.05\n",
    "num_heads = 8\n",
    "rate = 0.1\n",
    "num_projectors = 32\n",
    "NUM_EPOCHS = [80, 15, 20]\n",
    "lr = 0.001\n",
    "CUDA_DEVICE_ID = 0\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298dd3d8-ff9c-4c6a-b141-49f42344956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprocessors = [\n",
    "    lambda: AutosomeDataProcessor(\n",
    "    path_to_training_data=TRAIN_DATA_PATH,\n",
    "    path_to_validation_data=VALID_DATA_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE, \n",
    "    batch_per_epoch=BATCH_PER_EPOCH,\n",
    "    train_workers=N_PROCS,\n",
    "    valid_batch_size=VALID_BATCH_SIZE,\n",
    "    valid_workers=N_PROCS,\n",
    "    shuffle_train=True,\n",
    "    shuffle_val=False,\n",
    "    plasmid_path=PLASMID_PATH,\n",
    "    seqsize=SEQ_SIZE,\n",
    "    generator=generator\n",
    "    ), \n",
    "    lambda: BHIDataProcessor(\n",
    "    path_to_training_data=TRAIN_DATA_PATH,\n",
    "    path_to_validation_data=VALID_DATA_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE, \n",
    "    train_workers=N_PROCS,\n",
    "    valid_batch_size=VALID_BATCH_SIZE,\n",
    "    valid_workers=N_PROCS,\n",
    "    shuffle_train=True,\n",
    "    shuffle_val=False,\n",
    "    plasmid_path=PLASMID_PATH,\n",
    "    seqsize=SEQ_SIZE,\n",
    "    generator=generator\n",
    "    ), \n",
    "    lambda: UnlockDNADataProcessor(\n",
    "    path_to_training_data = TRAIN_DATA_PATH,\n",
    "    path_to_validation_data= VALID_DATA_PATH,\n",
    "    generator=generator,\n",
    "    head_len = HEAD_LEN,\n",
    "    tail_len = TAIL_LEN,\n",
    "    max_width = SEQ_SIZE//2,\n",
    "    train_batch_size = TRAIN_BATCH_SIZE,\n",
    "    train_workers =N_PROCS,\n",
    "    shuffle_train =True,\n",
    "    valid_batch_size=VALID_BATCH_SIZE,\n",
    "    valid_workers=N_PROCS,\n",
    "    shuffle_val=False)\n",
    "]\n",
    "\n",
    "firsts = [\n",
    "    lambda: AutosomeFirstLayersBlock(in_channels=dataprocessor.data_channels(),\n",
    "                                   out_channels=256, \n",
    "                                   seqsize=dataprocessor.data_seqsize()),\n",
    "    \n",
    "    lambda: BHIFirstLayersBlock(\n",
    "    in_channels = dataprocessor.data_channels(),\n",
    "    out_channels = 320,\n",
    "    seqsize = dataprocessor.data_seqsize(),\n",
    "    kernel_sizes = [9, 15],\n",
    "    pool_size = 1,\n",
    "    dropout = 0.2\n",
    "    ),\n",
    "    \n",
    "    lambda: UnlockDNAFirstLayersBlock(in_channels = dataprocessor.data_channels(),\n",
    "        out_channels = 512,\n",
    "        seqsize = dataprocessor.data_seqsize(),\n",
    "        kmer = kmer,\n",
    "        strides = strides,\n",
    "        num_projectors = num_projectors)\n",
    "]\n",
    "\n",
    "cores = [\n",
    "    lambda: AutosomeCoreBlock(in_channels=first.out_channels,\n",
    "                         out_channels =64,\n",
    "                         seqsize=first.infer_outseqsize()),\n",
    "    \n",
    "    lambda: BHICoreBlock(\n",
    "    in_channels = first.out_channels,\n",
    "    out_channels = 320,\n",
    "    seqsize = first.infer_outseqsize(),\n",
    "    lstm_hidden_channels = 320,\n",
    "    kernel_sizes = [9, 15],\n",
    "    pool_size = 1,\n",
    "    dropout1 = 0.2,\n",
    "    dropout2 = 0.5\n",
    "    ),\n",
    "    \n",
    "    lambda: UnlockDNACoreBlock(\n",
    "    in_channels = first.out_channels, out_channels= first.out_channels, seqsize = dataprocessor.data_seqsize(), n_blocks = n_blocks,\n",
    "                                     kernel_size = 15, rate = rate, num_heads = num_heads)\n",
    "]\n",
    "\n",
    "finals = [\n",
    "    lambda: AutosomeFinalLayersBlock(in_channels=core.out_channels,\n",
    "                                 seqsize=core.infer_outseqsize()),\n",
    "    \n",
    "    lambda: BHIFinalLayersBlock(\n",
    "    in_channels = core.out_channels,\n",
    "    seqsize = dataprocessor.data_seqsize(),\n",
    "    hidden_dim = 64,\n",
    "    ),\n",
    "    \n",
    "    lambda: UnlockDNAFinalLayersBlock(\n",
    "    in_channels = core.out_channels,\n",
    "    seqsize = dataprocessor.data_seqsize(),\n",
    "    num_projectors = num_projectors,\n",
    "    input_dim = input_dim,\n",
    "    rate = rate)\n",
    "]\n",
    "\n",
    "trainers = [\n",
    "    lambda: AutosomeTrainer(\n",
    "    model,    \n",
    "    device=torch.device(f\"cuda:{CUDA_DEVICE_ID}\"), \n",
    "    model_dir=MODEL_LOG_DIR,\n",
    "    dataprocessor=dataprocessor,\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    lr = lr),\n",
    "    \n",
    "    lambda: BHITrainer(\n",
    "    model,    \n",
    "    device=torch.device(f\"cuda:{CUDA_DEVICE_ID}\"), \n",
    "    model_dir=MODEL_LOG_DIR,\n",
    "    dataprocessor=dataprocessor,\n",
    "    num_epochs=NUM_EPOCH\n",
    "    ),\n",
    "    \n",
    "    lambda: UnlockDNATrainer(\n",
    "    model,\n",
    "    device=torch.device(f\"cuda:{CUDA_DEVICE_ID}\"),\n",
    "    model_dir=MODEL_LOG_DIR,\n",
    "    dataprocessor=dataprocessor,\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    initial_lr = 1e-14,\n",
    "    embedding_dim = 256,\n",
    "    warmup_steps = 12500,\n",
    "    beta_1 =  0.9,\n",
    "    beta_2 = 0.98,\n",
    "    eps = 1e-9,\n",
    "    clip_norm = 1.,\n",
    "    clip_value = 0.5,\n",
    "    n_positions = SEQ_SIZE,\n",
    "    N = 4,\n",
    "    M = 5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c70adb-3f8c-4cef-8816-55a7af90d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = 'prix_fixe_model_weights'\n",
    "model_ids = os.listdir(direc)\n",
    "\n",
    "# filtered_model_ids = []\n",
    "# for model_id in model_ids:\n",
    "#     if model_id.split('_')[0] == '1':\n",
    "#         filtered_model_ids.append(model_id)\n",
    "\n",
    "job_ids = []\n",
    "for dataprocessor_id in range(3):\n",
    "    for id, (first, core, final) in enumerate(itertools.product(firsts, cores, finals)):\n",
    "        id_0, id_1, id_2 = firsts.index(first), cores.index(core), finals.index(final)\n",
    "        model_id = f\"{dataprocessor_id}_{id_0}_{id_1}_{id_2}\"\n",
    "        if model_id not in model_ids:\n",
    "            continue\n",
    "        job_ids.append(dataprocessor_id * 27 + id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1441ab-907a-4e09-a6f7-63f2c3ffdeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be953c2-6a30-4cfd-8638-f01c8232b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_0_0_0 successful\n",
      "0_0_0_1 successful\n",
      "0_0_1_0 successful\n",
      "0_0_1_1 successful\n",
      "0_0_2_0 successful\n",
      "0_0_2_1 successful\n",
      "0_1_0_0 successful\n",
      "0_1_0_1 successful\n",
      "0_1_1_0 successful\n",
      "0_1_1_1 successful\n",
      "0_1_2_0 successful\n",
      "0_1_2_1 successful\n",
      "0_2_0_0 successful\n",
      "0_2_0_1 successful\n",
      "0_2_1_0 successful\n",
      "0_2_1_1 successful\n",
      "0_2_2_0 successful\n",
      "0_2_2_1 successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 6370/6370 [00:00<00:00, 28139.52it/s]\n",
      "100%|████████████████████████████████████| 6370/6370 [00:00<00:00, 27873.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_0_0_0 successful\n",
      "1_0_0_1 successful\n",
      "1_0_1_0 successful\n",
      "1_0_1_1 successful\n",
      "1_0_2_0 successful\n",
      "1_1_0_0 successful\n",
      "1_1_0_1 successful\n",
      "1_1_1_0 successful\n",
      "1_1_1_1 successful\n",
      "1_1_2_0 successful\n",
      "1_2_0_0 successful\n",
      "1_2_0_1 successful\n",
      "1_2_1_0 successful\n",
      "1_2_1_1 successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muntakimrafi/anaconda3/envs/dream/lib/python3.10/site-packages/numpy/lib/shape_base.py:790: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  sub_arys.append(_nx.swapaxes(sary[st:end], axis, 0))\n",
      "/home/muntakimrafi/anaconda3/envs/dream/lib/python3.10/site-packages/numpy/lib/shape_base.py:790: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  sub_arys.append(_nx.swapaxes(sary[st:end], axis, 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_0_0_2 successful\n",
      "2_0_1_2 successful\n",
      "2_0_2_2 failed\n",
      "2_1_0_2 successful\n",
      "2_1_1_2 successful\n",
      "2_1_2_2 failed\n",
      "2_2_0_2 successful\n",
      "2_2_1_2 successful\n",
      "2_2_2_2 failed\n"
     ]
    }
   ],
   "source": [
    "for dataprocessor_id in range(3):\n",
    "    SEQ_SIZE = SEQ_SIZES[dataprocessor_id]\n",
    "    TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZES[dataprocessor_id]\n",
    "    VALID_BATCH_SIZE = VALID_BATCH_SIZES[dataprocessor_id]\n",
    "    BATCH_PER_EPOCH = BATCH_PER_EPOCHS[dataprocessor_id]\n",
    "    BATCH_PER_VALIDATION = BATCH_PER_VALIDATIONS[dataprocessor_id]\n",
    "    lr = lr\n",
    "    NUM_EPOCH = NUM_EPOCHS[dataprocessor_id]\n",
    "    dataprocessor = dataprocessors[dataprocessor_id]()\n",
    "\n",
    "    for id, (first, core, final) in enumerate(itertools.product(firsts, cores, finals)):\n",
    "\n",
    "        check = dataprocessor_id * 27 + id\n",
    "        if check not in job_ids:\n",
    "            continue\n",
    "        id_0, id_1, id_2 = firsts.index(first), cores.index(core), finals.index(final)\n",
    "\n",
    "        first = first()\n",
    "        core = core()\n",
    "        final = final()\n",
    "\n",
    "        model = PrixFixeNet(\n",
    "        first=first,\n",
    "        core=core,\n",
    "        final=final,\n",
    "        generator=generator\n",
    "        )\n",
    "        # from torchinfo import summary\n",
    "        # print(summary(model, (1, dataprocessor.data_channels(), dataprocessor.data_seqsize())))\n",
    "        \n",
    "        try:\n",
    "            MODEL_LOG_DIR = f\"prix_fixe_model_weights/{dataprocessor_id}_{id_0}_{id_1}_{id_2}\"\n",
    "            model.load_state_dict(torch.load(os.path.join(MODEL_LOG_DIR, 'model_best.pth')))\n",
    "            print(f\"{dataprocessor_id}_{id_0}_{id_1}_{id_2} successful\")\n",
    "        except:\n",
    "            print(f\"{dataprocessor_id}_{id_0}_{id_1}_{id_2} failed\")            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"markdown","source":["# Model training"],"metadata":{"id":"dV8UyqeaAmfv"}},{"cell_type":"markdown","source":["## Installs"],"metadata":{"id":"n6vfrzViAhmQ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7999,"status":"ok","timestamp":1659607386522,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"},"user_tz":-120},"id":"r4gbvGtNlC8a","outputId":"5cb82f6d-9190-427f-ffc3-b559aa9621c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dm-sonnet\n","  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\n","\u001b[K     |████████████████████████████████| 254 kB 5.1 MB/s \n","\u001b[?25hCollecting folium==0.2.1\n","  Downloading folium-0.2.1.tar.gz (69 kB)\n","\u001b[K     |████████████████████████████████| 69 kB 6.8 MB/s \n","\u001b[?25hCollecting imgaug==0.2.6\n","  Downloading imgaug-0.2.6.tar.gz (631 kB)\n","\u001b[K     |████████████████████████████████| 631 kB 78.9 MB/s \n","\u001b[?25hRequirement already satisfied: Jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (2.11.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.7.3)\n","Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (0.18.3)\n","Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.15.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2==2.11.3) (2.0.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (1.3.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.6.3)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (7.1.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.9.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2021.11.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (4.1.1)\n","Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (0.1.7)\n","Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (1.2.0)\n","Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (0.8.10)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet) (1.14.1)\n","Building wheels for collected packages: folium, imgaug\n","  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=416c74272a01b4e97bfc67a95ec430b24e3a96cab990eaf9e284eeaaa7d6f716\n","  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654017 sha256=c09e2b74bd5038307dde53cd093911859fc22f89474356ce9b90ce40e0af0668\n","  Stored in directory: /root/.cache/pip/wheels/89/72/98/3ebfdba1069a9a8eaaa7ae7265cfd67d63ef0197aaee2e5f9c\n","Successfully built folium imgaug\n","Installing collected packages: imgaug, folium, dm-sonnet\n","  Attempting uninstall: imgaug\n","    Found existing installation: imgaug 0.4.0\n","    Uninstalling imgaug-0.4.0:\n","      Successfully uninstalled imgaug-0.4.0\n","  Attempting uninstall: folium\n","    Found existing installation: folium 0.12.1.post1\n","    Uninstalling folium-0.12.1.post1:\n","      Successfully uninstalled folium-0.12.1.post1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.17.5 requires folium>=0.9.1, but you have folium 0.2.1 which is incompatible.\u001b[0m\n","Successfully installed dm-sonnet-2.0.0 folium-0.2.1 imgaug-0.2.6\n"]}],"source":["!pip install dm-sonnet folium==0.2.1 imgaug==0.2.6 Jinja2==2.11.3\n","#!pip install einops\n","#!pip install tensorflow-addons"]},{"cell_type":"markdown","source":["## Google cloud authentication"],"metadata":{"id":"70F6TRE6Ar5g"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16459,"status":"ok","timestamp":1659607402977,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"},"user_tz":-120},"id":"bsXY8LlwoJIn","outputId":"49812511-0b4d-4069-87c9-a35508b4d481"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"nVZ2U--doQVs","executionInfo":{"status":"ok","timestamp":1659607416009,"user_tz":-120,"elapsed":13035,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["from google.colab import auth\n","auth.authenticate_user()"]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"E_IuSpbmAyej"}},{"cell_type":"code","execution_count":53,"metadata":{"id":"ky_7Sbo8nxEr","executionInfo":{"status":"ok","timestamp":1659614419667,"user_tz":-120,"elapsed":194,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["import tensorflow.keras as keras\n","import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.python.client import device_lib\n","from tensorflow.keras import Input\n","from tensorflow.keras.initializers import Identity, glorot_uniform, Zeros, he_normal\n","from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Conv1D, BatchNormalization, Add , Activation , Lambda , Dropout, LayerNormalization, Input, Layer\n","from tensorflow.keras.regularizers import l1_l2, l2, l1\n","from tensorflow.keras.backend import conv1d\n","from tensorflow.keras.models import Model\n","from tensorflow.python.keras.utils import conv_utils\n","from tensorflow.keras.activations import gelu, relu, swish\n","from tensorflow.keras import backend as K\n","import tensorflow as tf\n","import networkx as nx\n","import scipy\n","from sklearn.preprocessing import LabelEncoder\n","import logging\n","import numpy as np\n","import pandas as pd\n","import h5py\n","import os\n","import datetime\n","import csv"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1659607419239,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"},"user_tz":-120},"id":"GqrrOETGoOTO"},"outputs":[],"source":["import math\n","#from einops.layers.tensorflow import Rearrange, Reduce"]},{"cell_type":"markdown","source":["## Setting up TPU access"],"metadata":{"id":"uZ8Y_vvwBETk"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8847,"status":"ok","timestamp":1659607434096,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"},"user_tz":-120},"id":"1K43Y_1cofV1","outputId":"d7febedd-6f1e-4bf1-ed36-060b583453e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.1.0.146:8470\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.1.0.146:8470\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.tpu.topology.Topology at 0x7fd0d73bfc10>"]},"metadata":{},"execution_count":6}],"source":["#Get a handle to the attached TPU. On GCP it will be the CloudTPU itself\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","#Connect to the TPU handle and initialise it\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1659607434096,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"},"user_tz":-120},"id":"zov7mGgJoheq","outputId":"96d1b969-e087-4c4a-b70f-1a76ba664703"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]}],"source":["strategy = tf.distribute.TPUStrategy(resolver)"]},{"cell_type":"markdown","source":["## Model code"],"metadata":{"id":"s7Z42HafBKJS"}},{"cell_type":"code","source":["wd = '/content/drive/MyDrive/Colab Notebooks/DREAM/Final/'"],"metadata":{"id":"95gFWtpeEgXX","executionInfo":{"status":"ok","timestamp":1659607436332,"user_tz":-120,"elapsed":293,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"rvoJqJCIolZi","executionInfo":{"status":"ok","timestamp":1659607436532,"user_tz":-120,"elapsed":2,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["\n","#import sys\n","#sys.path.insert(1, wd)\n","\n","#from helpers import *"]},{"cell_type":"markdown","source":["### Enformer attention module"],"metadata":{"id":"bnHGTcXRsk0u"}},{"cell_type":"code","source":["## Copy-pasted from here: https://github.com/deepmind/deepmind-research/blob/master/enformer/attention_module.py\n","from typing import Any, Dict, List, Optional\n","\n","import numpy as np\n","import sonnet as snt\n","import tensorflow as tf\n","\n","class MultiheadAttention(snt.Module):\n","  \"\"\"Multi-head attention.\"\"\"\n","\n","  def __init__(self,\n","               value_size: int,\n","               key_size: int,\n","               num_heads: int,\n","               scaling: bool = True,\n","               attention_dropout_rate: float = 0.1,\n","               relative_positions: bool = False,\n","               relative_position_symmetric: bool = False,\n","               relative_position_functions: Optional[List[str]] = None,\n","               num_relative_position_features: Optional[int] = None,\n","               positional_dropout_rate: float = 0.1,\n","               zero_initialize: bool = True,\n","               initializer: Optional[snt.initializers.Initializer] = None,\n","               name: str = None):\n","    \"\"\"Creates a MultiheadAttention module.\n","    Args:\n","      value_size: The size of each value embedding per head.\n","      key_size: The size of each key and query embedding per head.\n","      num_heads: The number of independent queries per timestep.\n","      scaling: Whether to scale the attention logits.\n","      attention_dropout_rate: Dropout rate for attention logits.\n","      relative_positions: Whether to use TransformerXL style relative attention.\n","      relative_position_symmetric: If True, the symmetric version of basis\n","        functions will be used. If False, a symmetric and asymmetric versions\n","        will be use.\n","      relative_position_functions: List of function names used for relative\n","        positional biases.\n","      num_relative_position_features: Number of relative positional features\n","        to compute. If None, `value_size * num_heads` is used.\n","      positional_dropout_rate: Dropout rate for the positional encodings if\n","        relative positions are used.\n","      zero_initialize: if True, the final linear layer will be 0 initialized.\n","      initializer: Initializer for the projection layers. If unspecified,\n","        VarianceScaling is used with scale = 2.0.\n","      name: Name of module.\n","    \"\"\"\n","    super().__init__(name=name)\n","    self._value_size = value_size\n","    self._key_size = key_size\n","    self._num_heads = num_heads\n","    self._attention_dropout_rate = attention_dropout_rate\n","    self._scaling = scaling\n","    self._relative_positions = relative_positions\n","    self._relative_position_symmetric = relative_position_symmetric\n","    self._relative_position_functions = relative_position_functions\n","    if num_relative_position_features is None:\n","      # num_relative_position_features needs to be divisible by the number of\n","      # relative positional functions *2 (for symmetric & asymmetric version).\n","      divisible_by = 2 * len(self._relative_position_functions)\n","      self._num_relative_position_features = (\n","          (self._value_size // divisible_by) * divisible_by)\n","    else:\n","      self._num_relative_position_features = num_relative_position_features\n","    self._positional_dropout_rate = positional_dropout_rate\n","\n","    self._initializer = initializer\n","    if self._initializer is None:\n","      self._initializer = snt.initializers.VarianceScaling(scale=2.0)\n","\n","    key_proj_size = self._key_size * self._num_heads\n","    embedding_size = self._value_size * self._num_heads\n","\n","    self._q_layer = snt.Linear(\n","        key_proj_size,\n","        name='q_layer',\n","        with_bias=False,\n","        w_init=self._initializer)\n","    self._k_layer = snt.Linear(\n","        key_proj_size,\n","        name='k_layer',\n","        with_bias=False,\n","        w_init=self._initializer)\n","    self._v_layer = snt.Linear(\n","        embedding_size,\n","        name='v_layer',\n","        with_bias=False,\n","        w_init=self._initializer)\n","    w_init = snt.initializers.Zeros() if zero_initialize else self._initializer\n","    self._embedding_layer = snt.Linear(\n","        embedding_size,\n","        name='embedding_layer',\n","        w_init=w_init)\n","\n","    # Create additional layers if using relative positions.\n","    if self._relative_positions:\n","      self._r_k_layer = snt.Linear(\n","          key_proj_size,\n","          name='r_k_layer',\n","          with_bias=False,\n","          w_init=self._initializer)\n","      self._r_w_bias = tf.Variable(\n","          self._initializer([1, self._num_heads, 1, self._key_size],\n","                            dtype=tf.float32),\n","          name='r_w_bias')\n","      self._r_r_bias = tf.Variable(\n","          self._initializer([1, self._num_heads, 1, self._key_size],\n","                            dtype=tf.float32),\n","          name='r_r_bias')\n","\n","  def _multihead_output(self, linear, inputs):\n","    \"\"\"Applies a standard linear to inputs and returns multihead output.\"\"\"\n","\n","    output = snt.BatchApply(linear)(inputs)  # [B, T, H * KV]\n","    num_kv_channels = output.shape[-1] // self._num_heads\n","    # Split H * Channels into separate axes.\n","    output = snt.reshape(output,\n","                         output_shape=[-1, self._num_heads, num_kv_channels])\n","    # [B, T, H, KV] -> [B, H, T, KV]\n","    return tf.transpose(output, [0, 2, 1, 3])\n","\n","  def __call__(self,\n","               inputs,\n","               is_training=False):\n","    # Initialise the projection layers.\n","    embedding_size = self._value_size * self._num_heads\n","    seq_len = inputs.shape[1]\n","\n","    # Compute q, k and v as multi-headed projections of the inputs.\n","    q = self._multihead_output(self._q_layer, inputs)  # [B, H, T, K]\n","    k = self._multihead_output(self._k_layer, inputs)  # [B, H, T, K]\n","    v = self._multihead_output(self._v_layer, inputs)  # [B, H, T, V]\n","\n","    # Scale the query by the square-root of key size.\n","    if self._scaling:\n","      q *= self._key_size**-0.5\n","\n","    if self._relative_positions:\n","      # For relative positions, we project positions to form relative keys.\n","      distances = tf.range(-seq_len + 1, seq_len, dtype=tf.float32)[tf.newaxis]\n","      positional_encodings = positional_features_all(\n","          positions=distances,\n","          feature_size=self._num_relative_position_features,\n","          seq_length=seq_len,\n","          feature_functions=self._relative_position_functions,\n","          symmetric=self._relative_position_symmetric)\n","      # [1, 2T-1, Cr]\n","\n","      if is_training:\n","        positional_encodings = tf.nn.dropout(\n","            positional_encodings, rate=self._positional_dropout_rate)\n","\n","      # [1, H, 2T-1, K]\n","      r_k = self._multihead_output(self._r_k_layer, positional_encodings)\n","\n","      # Add shifted relative logits to content logits.\n","      # [B, H, T', T]\n","      content_logits = tf.matmul(q + self._r_w_bias, k, transpose_b=True)\n","      # [B, H, T', 2T-1]\n","      relative_logits = tf.matmul(\n","          q + self._r_r_bias, r_k, transpose_b=True)\n","      #  [B, H, T', T]\n","      relative_logits = relative_shift(relative_logits)\n","      logits = content_logits + relative_logits\n","    else:\n","      # [B, H, T', T]\n","      logits = tf.matmul(q, k, transpose_b=True)\n","\n","    weights = tf.nn.softmax(logits)\n","\n","    # Dropout on the attention weights.\n","    if is_training:\n","      weights = tf.nn.dropout(weights, rate=self._attention_dropout_rate)\n","\n","    # Transpose and reshape the output.\n","    output = tf.matmul(weights, v)  # [B, H, T', V]\n","    output_transpose = tf.transpose(output, [0, 2, 1, 3])  # [B, T', H, V]\n","\n","    # Final linear layer.\n","    attended_inputs = snt.reshape(\n","        output_transpose, output_shape=[embedding_size], preserve_dims=2)\n","    output = self._embedding_layer(attended_inputs)\n","\n","    return output\n","\n","\n","def relative_shift(x):\n","  \"\"\"Shift the relative logits like in TransformerXL.\"\"\"\n","  # We prepend zeros on the final timescale dimension.\n","  to_pad = tf.zeros_like(x[..., :1])\n","  x = tf.concat([to_pad, x], -1)\n","  _, num_heads, t1, t2 = x.shape\n","  x = tf.reshape(x, [-1, num_heads, t2, t1])\n","  x = tf.slice(x, [0, 0, 1, 0], [-1, -1, -1, -1])\n","  x = tf.reshape(x, [-1, num_heads, t1, t2 - 1])\n","  x = tf.slice(x, [0, 0, 0, 0], [-1, -1, -1, (t2 + 1) // 2])\n","  return x\n","\n","\n","# Available feature functions:\n","def get_positional_feature_function(name):\n","  \"\"\"Returns positional feature functions.\"\"\"\n","  available = {\n","      'positional_features_exponential': positional_features_exponential,\n","      'positional_features_central_mask': positional_features_central_mask,\n","      'positional_features_gamma': positional_features_gamma,\n","      'positional_features_cosine': positional_features_cosine,\n","      'positional_features_linear_masks': positional_features_linear_masks,\n","      'positional_features_sin_cos': positional_features_sin_cos,\n","  }\n","  if name not in available:\n","    raise ValueError(f'Function {name} not available in {available.keys()}')\n","  return available[name]\n","\n","\n","def positional_features_all(positions: tf.Tensor,\n","                            feature_size: int,\n","                            seq_length: Optional[int] = None,\n","                            bin_size: Optional[int] = None,\n","                            feature_functions: Optional[List[str]] = None,\n","                            symmetric=False):\n","  \"\"\"Compute relative positional encodings/features.\n","  Each positional feature function will compute/provide the same fraction of\n","  features, making up the total of feature_size.\n","  Args:\n","    positions: Tensor of relative positions of arbitrary shape.\n","    feature_size: Total number of basis functions.\n","    seq_length: Sequence length denoting the characteristic length that\n","      the individual positional features can use. This is required since the\n","      parametrization of the input features should be independent of `positions`\n","      while it could still require to use the total number of features.\n","    bin_size: Bin sized used to partition the sequence. This can be used to\n","      compute features on the absolute scale relative to the genome.\n","    feature_functions: List of different feature functions to use. Each function\n","      will take as argument: positions, sequence length and number of features\n","      to compute.\n","    symmetric: If True, the resulting features will be symmetric across the\n","      relative position of 0 (i.e. only absolute value of positions will\n","      matter). If false, then both the symmetric and asymmetric version\n","      (symmetric multiplied by sign(positions)) of the features will be used.\n","  Returns:\n","    Tensor of shape: `positions.shape + (feature_size,)`.\n","  \"\"\"\n","  if feature_functions is None:\n","    feature_functions = ['positional_features_exponential',\n","                         'positional_features_central_mask',\n","                         'positional_features_gamma']\n","  num_components = len(feature_functions)  # 1 per each basis function\n","  if not symmetric:\n","    num_components = 2 * num_components\n","\n","  # For now, we do not allow odd sized embeddings.\n","  if feature_size % num_components != 0:\n","    raise ValueError(\n","        f'feature_size has to be divisible by {num_components}')\n","\n","  feature_functions = [get_positional_feature_function(f)\n","                       for f in feature_functions]\n","  num_basis_per_class = feature_size // num_components\n","  embeddings = tf.concat([f(tf.abs(positions), num_basis_per_class,\n","                            seq_length, bin_size)\n","                          for f in feature_functions],\n","                         axis=-1)\n","  if not symmetric:\n","    embeddings = tf.concat([embeddings,\n","                            tf.sign(positions)[..., tf.newaxis] * embeddings],\n","                           axis=-1)\n","  tf.TensorShape(embeddings.shape).assert_is_compatible_with(\n","      positions.shape + [feature_size])\n","  return embeddings\n","\n","\n","def _prepend_dims(x, num_dims):\n","  return tf.reshape(x, shape=[1] * num_dims + x.shape)\n","\n","\n","def positional_features_exponential(positions: tf.Tensor,\n","                                    feature_size: int,\n","                                    seq_length: Optional[int] = None,\n","                                    bin_size: Optional[int] = None,\n","                                    min_half_life: Optional[float] = 3.0):\n","  \"\"\"Create exponentially decaying positional weights.\n","  Args:\n","    positions: Position tensor (arbitrary shape).\n","    feature_size: Number of basis functions to use.\n","    seq_length: Sequence length.\n","    bin_size: (unused). See `positional_features_all`.\n","    min_half_life: Smallest exponential half life in the grid of half lives.\n","  Returns:\n","    A Tensor with shape [2 * seq_length - 1, feature_size].\n","  \"\"\"\n","  del bin_size  # Unused.\n","  if seq_length is None:\n","    seq_length = tf.reduce_max(tf.abs(positions)) + 1\n","  # Grid of half lifes from [3, seq_length / 2] with feature_size\n","  # distributed on the log scale.\n","  seq_length = tf.cast(seq_length, dtype=tf.float32)\n","  max_range = tf.math.log(seq_length) / tf.math.log(2.0)\n","  half_life = tf.pow(2.0, tf.linspace(min_half_life, max_range, feature_size))\n","  half_life = _prepend_dims(half_life, positions.shape.rank)\n","  positions = tf.abs(positions)\n","  outputs = tf.exp(-tf.math.log(2.0) / half_life * positions[..., tf.newaxis])\n","  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n","      positions.shape + [feature_size])\n","  return outputs\n","\n","\n","def positional_features_central_mask(positions: tf.Tensor,\n","                                     feature_size: int,\n","                                     seq_length: Optional[int] = None,\n","                                     bin_size: Optional[int] = None):\n","  \"\"\"Positional features using a central mask (allow only central features).\"\"\"\n","  del seq_length  # Unused.\n","  del bin_size  # Unused.\n","  center_widths = tf.pow(2.0, tf.range(1, feature_size + 1, dtype=tf.float32))\n","  center_widths = center_widths - 1\n","  center_widths = _prepend_dims(center_widths, positions.shape.rank)\n","  outputs = tf.cast(center_widths > tf.abs(positions)[..., tf.newaxis],\n","                    tf.float32)\n","  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n","      positions.shape + [feature_size])\n","  return outputs\n","\n","\n","def gamma_pdf(x, concentration, rate):\n","  \"\"\"Gamma probability distribution function: p(x|concentration, rate).\"\"\"\n","  log_unnormalized_prob = tf.math.xlogy(concentration - 1., x) - rate * x\n","  log_normalization = (tf.math.lgamma(concentration) -\n","                       concentration * tf.math.log(rate))\n","  return tf.exp(log_unnormalized_prob - log_normalization)\n","\n","\n","def positional_features_gamma(positions: tf.Tensor,\n","                              feature_size: int,\n","                              seq_length: Optional[int] = None,\n","                              bin_size: Optional[int] = None,\n","                              stddev=None,\n","                              start_mean=None):\n","  \"\"\"Positional features computed using the gamma distributions.\"\"\"\n","  del bin_size  # Unused.\n","  if seq_length is None:\n","    seq_length = tf.reduce_max(tf.abs(positions)) + 1\n","  if stddev is None:\n","    stddev = seq_length / (2 * feature_size)\n","  if start_mean is None:\n","    start_mean = seq_length / feature_size\n","  mean = tf.linspace(start_mean, seq_length, num=feature_size)\n","  mean = _prepend_dims(mean, positions.shape.rank)\n","  concentration = (mean / stddev)**2\n","  rate = mean / stddev**2\n","  probabilities = gamma_pdf(\n","      tf.abs(tf.cast(positions, dtype=tf.float32))[..., tf.newaxis],\n","      concentration, rate)\n","  probabilities += 1e-8  # To ensure numerical stability.\n","  outputs = probabilities / tf.reduce_max(probabilities,\n","                                          axis=1, keepdims=True)\n","  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n","      positions.shape + [feature_size])\n","  return outputs\n","\n","\n","def positional_features_cosine(positions: tf.Tensor,\n","                               feature_size: int,\n","                               seq_length: Optional[int] = None,\n","                               bin_size: Optional[int] = None):\n","  \"\"\"Cosine positional features.\"\"\"\n","  del bin_size  # Unused.\n","  del seq_length  # Unused.\n","  periodicity = 1.25 * tf.pow(2.0, tf.range(0, feature_size, dtype=tf.float32))\n","  periodicity = _prepend_dims(periodicity, positions.shape.rank)\n","\n","  outputs = tf.math.cos(2 * np.pi * positions[..., tf.newaxis] / periodicity)\n","  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n","      positions.shape + [feature_size])\n","  return outputs\n","\n","\n","def positional_features_linear_masks(positions: tf.Tensor,\n","                                     feature_size: int,\n","                                     seq_length: Optional[int] = None,\n","                                     bin_size: Optional[int] = None):\n","  \"\"\"Exponentially increasing point focuses.\"\"\"\n","  del bin_size  # Unused.\n","  del seq_length  # Unused.\n","  distances = tf.range(0, feature_size, dtype=tf.float32)\n","  distances = _prepend_dims(distances, positions.shape.rank)\n","  outputs = tf.cast(distances == tf.abs(positions[..., tf.newaxis]),\n","                    dtype=tf.float32)\n","\n","  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n","      positions.shape + [feature_size])\n","  return outputs\n","\n","\n","def positional_features_sin_cos(positions: tf.Tensor,\n","                                feature_size: int,\n","                                seq_length: Optional[int] = None,\n","                                bin_size: Optional[int] = None,\n","                                max_time=10000.0):\n","  \"\"\"Sine/cosine positional encodings.\"\"\"\n","  del bin_size  # Unused.\n","  del seq_length  # Unused.\n","  if feature_size % 2 != 0:\n","    raise ValueError('feature_size needs to be divisible by 2.')\n","  i = tf.range(0, feature_size, 2, dtype=tf.float32)\n","  i = _prepend_dims(i, positions.shape.rank)\n","\n","  # Concat sines and cosines and return.\n","  outputs = tf.concat([\n","      tf.sin(positions[..., tf.newaxis] / max_time**(i / feature_size)),\n","      tf.cos(positions[..., tf.newaxis] / max_time**(i / feature_size))], -1)\n","\n","  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n","      positions.shape + [feature_size])\n","  return outputs"],"metadata":{"id":"4_KI6P7oxVe5","executionInfo":{"status":"ok","timestamp":1659607439863,"user_tz":-120,"elapsed":1557,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yK2m6_LAooMc"},"source":["## Loading data"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"5M1crLC8QwYW","executionInfo":{"status":"ok","timestamp":1659607457624,"user_tz":-120,"elapsed":16911,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["with h5py.File(wd + 'train_onehot_sequences_bool_half.h5', 'r') as hf:\n","    train_sequences = hf['onehot_sequences_bool'][:] \n","\n","with h5py.File(wd + 'train_expression_half.h5', 'r') as hf:\n","    train_expression = hf['expression'][:] "]},{"cell_type":"code","source":["# Define train-validation sizes \n","Total_sequences = len(train_expression) \n","train_split = 0.98\n","validation_split = 0.02\n","num_train = int(Total_sequences * train_split)\n","num_validation = int(Total_sequences * validation_split)"],"metadata":{"id":"nWCR-whAIP5b","executionInfo":{"status":"ok","timestamp":1659607457624,"user_tz":-120,"elapsed":12,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["print('Training sequences : {}'.format(num_train))\n","print('Validation sequences : {}'.format(num_validation))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFkM50uZ4Lmy","executionInfo":{"status":"ok","timestamp":1659607457625,"user_tz":-120,"elapsed":12,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}},"outputId":"8d278b21-6278-4dcf-ea5e-cf5f3a4c6cb8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training sequences : 4703150\n","Validation sequences : 95982\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"UdaEyMp14L4M","executionInfo":{"status":"ok","timestamp":1659607457625,"user_tz":-120,"elapsed":3,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"BpdDrOjoQwU2","executionInfo":{"status":"ok","timestamp":1659607472945,"user_tz":-120,"elapsed":212,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["# Make TF dataset and do shuffling and splits\n","full_dataset = tf.data.Dataset.from_tensor_slices((train_sequences, train_expression))\n","\n","full_dataset = full_dataset.shuffle(buffer_size = 1024)\n","train_dataset = full_dataset.take(num_train)\n","validation_dataset = full_dataset.skip(num_train)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"OTbJsrQaQwN7","executionInfo":{"status":"ok","timestamp":1659607506539,"user_tz":-120,"elapsed":33596,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["# Define train steps \n","N_train_batches = len(train_dataset) // 128\n","N_val_batches = len(validation_dataset) // 128"]},{"cell_type":"code","source":[""],"metadata":{"id":"f0OkixjH4MKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"1NCqkkrN4Mdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tUuIYcfo4MtQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-vYVe5DprCy"},"source":["## Model"]},{"cell_type":"markdown","source":["### Logging and reporting"],"metadata":{"id":"jnb7Ov6Wszhc"}},{"cell_type":"code","execution_count":23,"metadata":{"id":"MrBk56eIh4Vn","executionInfo":{"status":"ok","timestamp":1659607582898,"user_tz":-120,"elapsed":220,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["# Function to compute pearson correlation\n","def correlation_coefficient(y_true, y_pred): # Copied from https://github.com/1edv/evolution/blob/master/manuscript_code/model/tpu_model/rr_aux.py\n","    x = y_true\n","    y = y_pred\n","    mx = K.mean(x)\n","    my = K.mean(y)\n","    xm, ym = x-mx, y-my\n","    r_num = K.sum(tf.multiply(xm,ym))\n","    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n","    r = r_num / r_den\n","\n","    r = K.maximum(K.minimum(r, 1.0), -1.0)\n","    return K.square(r)\n","\n","def r_square(y_true, y_pred): # Copied from https://github.com/1edv/evolution/blob/master/manuscript_code/model/tpu_model/rr_aux.py\n","    SS_res =  K.sum(K.square(y_true - y_pred)) \n","    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n","    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"cxH7ZvLjrIPT","executionInfo":{"status":"ok","timestamp":1659607525227,"user_tz":-120,"elapsed":184,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["class LossHistory(keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","history = LossHistory()\n","\n","csvlogger = keras.callbacks.CSVLogger(wd+\"loss_history_Final_model.tsv\", separator='\\t', append=False)\n","\n","\n","model_path = wd + \"Final_model.h5\"\n","checkpoint = keras.callbacks.ModelCheckpoint(model_path, monitor='val_correlation_coefficient', \n","                                             verbose=2, save_best_only=True, mode='max', save_weights_only = True)\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_correlation_coefficient', patience=20, mode='max') \n","callbacks_list = [checkpoint, early_stop, history , csvlogger ]\n"]},{"cell_type":"markdown","metadata":{"id":"Cg1WES6MqklS"},"source":["### Model parameters and architecture"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"gg1NEKlXpsTD","executionInfo":{"status":"ok","timestamp":1659608848347,"user_tz":-120,"elapsed":564,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["batch_size= 1024 \n","input_shape = (batch_size // 8, 110, 4)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"8PXVMwW9baLB","executionInfo":{"status":"ok","timestamp":1659608848598,"user_tz":-120,"elapsed":2,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["# Training params\n","epochs = 10\n","batch_size= int(1024*1) \n","lr=0.0001\n","loss = 'mean_squared_error'\n","device_type ='tpu'\n","\n","# CNN params\n","conv_size0 = 15\n","conv_size1 = 20\n","conv_size2 = 25\n","conv_size3 = 30\n","reduce_hidden = 96\n","cnn_activation = 'swish'\n","cnn_initializer = 'he_normal'\n","\n","# LSTM params\n","lstm_activation = 'swish'\n","lstm_initializer = 'he_normal'\n","lstm_units = 4\n","\n","# Transformer params\n","num_heads = 1\n","transformer_att_dropout = 0.0\n","transformer_pos_dropout = 0.0\n","transformer_rel_pos = True\n","transformer_pos_fncs = ['positional_features_exponential','positional_features_central_mask','positional_features_gamma']\n","\n","# Output param\n","output_initializer = 'he_normal'\n","\n","# Regularization\n","l1_weight= 0\n","l2_weight= 0.001"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"InJj0CvHacL6","executionInfo":{"status":"ok","timestamp":1659608849116,"user_tz":-120,"elapsed":311,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"outputs":[],"source":["## Current best\n","def create_model():\n","  input_layer = Input(batch_shape=(int(batch_size / 8),input_shape[1],input_shape[2]))  #Batches are split into 8 \"blocks\" on the tpu\n","\n","  # Convolutional block\n","  x_0 = Conv1D(reduce_hidden * 2, kernel_size = conv_size0, padding='same' , kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer=cnn_initializer, data_format = 'channels_last')(input_layer) \n","  x_0 = BatchNormalization()(x_0)\n","  x_0 = Activation(cnn_activation)(x_0)\n","  x_0 = Conv1D(reduce_hidden, kernel_size = 1, padding='same' , kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer=cnn_initializer, data_format = 'channels_last')(x_0) \n","  x_0 = BatchNormalization()(x_0)\n","  x_0 = Activation(cnn_activation)(x_0)\n","\n","  x_1 = Conv1D(reduce_hidden * 2, kernel_size = conv_size1, padding='same' , kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer=cnn_initializer, data_format = 'channels_last')(input_layer) \n","  x_1 = BatchNormalization()(x_1)\n","  x_1 = Activation(cnn_activation)(x_1)\n","  x_1 = Conv1D(reduce_hidden, kernel_size = 1, padding='same' , kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer=cnn_initializer, data_format = 'channels_last')(x_1) \n","  x_1 = BatchNormalization()(x_1)\n","  x_1 = Activation(cnn_activation)(x_1)\n","\n","  x_2 = Conv1D(reduce_hidden * 2, kernel_size = conv_size2, padding='same' , kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer=cnn_initializer, data_format = 'channels_last')(input_layer) \n","  x_2 = BatchNormalization()(x_2)\n","  x_2 = Activation(cnn_activation)(x_2)\n","  x_2 = Conv1D(reduce_hidden, kernel_size = 1, padding='same' , kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer=cnn_initializer, data_format = 'channels_last')(x_2) \n","  x_2 = BatchNormalization()(x_2)\n","  x_2 = Activation(cnn_activation)(x_2)\n","\n","  x_3 = Conv1D(reduce_hidden * 2, kernel_size = conv_size3, padding='same' , kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer=cnn_initializer, data_format = 'channels_last')(input_layer) \n","  x_3 = BatchNormalization()(x_3)\n","  x_3 = Activation(cnn_activation)(x_3)\n","  x_3 = Conv1D(reduce_hidden, kernel_size = 1, padding='same' , kernel_regularizer = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer=cnn_initializer, data_format = 'channels_last')(x_3) \n","  x_3 = BatchNormalization()(x_3)\n","  x_3 = Activation(cnn_activation)(x_3)\n","\n","  x = Add()([x_0, x_1, x_2, x_3])\n","\n","  # Transformer block\n","  mha = LayerNormalization()(x)\n","  mha = MultiheadAttention(\n","      value_size = reduce_hidden // num_heads,\n","      key_size =  reduce_hidden // num_heads,\n","      num_heads = num_heads,\n","      attention_dropout_rate = transformer_att_dropout,\n","      relative_positions = transformer_rel_pos,\n","      relative_position_functions = transformer_pos_fncs,\n","      positional_dropout_rate = transformer_pos_dropout)(mha)\n","  mha = Add()([x, mha])\n","  lin = LayerNormalization()(mha)\n","  lin = snt.Linear(reduce_hidden*2)(lin)\n","  lin = Activation('swish')(lin)\n","  lin = snt.Linear(reduce_hidden)(lin)\n","\n","  x = Add()([mha, lin])\n","  mha = LayerNormalization()(x)\n","  mha = MultiheadAttention(\n","      value_size = reduce_hidden // num_heads,\n","      key_size =  reduce_hidden // num_heads,\n","      num_heads = num_heads,\n","      attention_dropout_rate = transformer_att_dropout,\n","      relative_positions = transformer_rel_pos,\n","      relative_position_functions = transformer_pos_fncs,\n","      positional_dropout_rate = transformer_pos_dropout)(mha)\n","  mha = Add()([x, mha])\n","  lin = LayerNormalization()(mha)\n","  lin = snt.Linear(reduce_hidden*2)(lin)\n","  lin = Activation('swish')(lin)\n","  lin = snt.Linear(reduce_hidden)(lin)\n","  x = Add()([mha, lin])\n","\n","  # LSTM\n","  x = Bidirectional(LSTM(lstm_units, \n","                         return_sequences=False,\n","                         kernel_initializer = lstm_initializer,\n","                         activation = lstm_activation))(x)\n","\n","  # Linear output layer\n","  output_layer = Dense(1, \n","                       kernel_regularizer = l1_l2(l1=l1_weight, l2=l2_weight),\n","                       activation='linear', \n","                       kernel_initializer=output_initializer, \n","                       use_bias=True )(x) \n","  \n","  model = Model(input_layer, output_layer)\n","  opt = tf.keras.optimizers.RMSprop(lr)\n","  return model, opt"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"IdZnztKOhQJH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659608851439,"user_tz":-120,"elapsed":2110,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}},"outputId":"8a19599e-cdd4-4a9a-8eee-6b69cc4b141a"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_54), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/q_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_54), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/q_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_55), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/k_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_55), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/k_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_56), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/v_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_56), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/v_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/r_w_bias/packed:0' shape=(1, 1, 1, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/r_w_bias/packed:0' shape=(1, 1, 1, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/r_r_bias/packed:0' shape=(1, 1, 1, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/r_r_bias/packed:0' shape=(1, 1, 1, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_60), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/embedding_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_60), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/embedding_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_18), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/embedding_layer/b/packed:0' shape=(96,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_18), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/embedding_layer/b/packed:0' shape=(96,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_61), but are not present in its tracked objects:   <tf.Variable 'linear/w/packed:0' shape=(96, 192) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_61), but are not present in its tracked objects:   <tf.Variable 'linear/w/packed:0' shape=(96, 192) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_19), but are not present in its tracked objects:   <tf.Variable 'linear/b/packed:0' shape=(192,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_19), but are not present in its tracked objects:   <tf.Variable 'linear/b/packed:0' shape=(192,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_62), but are not present in its tracked objects:   <tf.Variable 'linear/w/packed:0' shape=(192, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_62), but are not present in its tracked objects:   <tf.Variable 'linear/w/packed:0' shape=(192, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_20), but are not present in its tracked objects:   <tf.Variable 'linear/b/packed:0' shape=(96,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_20), but are not present in its tracked objects:   <tf.Variable 'linear/b/packed:0' shape=(96,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_63), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/q_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_63), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/q_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_64), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/k_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_64), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/k_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_65), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/v_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_65), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/v_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/r_w_bias/packed:0' shape=(1, 1, 1, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/r_w_bias/packed:0' shape=(1, 1, 1, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/r_r_bias/packed:0' shape=(1, 1, 1, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/r_r_bias/packed:0' shape=(1, 1, 1, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_69), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/embedding_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_69), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/embedding_layer/w/packed:0' shape=(96, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_21), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/embedding_layer/b/packed:0' shape=(96,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_21), but are not present in its tracked objects:   <tf.Variable 'multihead_attention/embedding_layer/b/packed:0' shape=(96,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_70), but are not present in its tracked objects:   <tf.Variable 'linear/w/packed:0' shape=(96, 192) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_70), but are not present in its tracked objects:   <tf.Variable 'linear/w/packed:0' shape=(96, 192) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_22), but are not present in its tracked objects:   <tf.Variable 'linear/b/packed:0' shape=(192,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_22), but are not present in its tracked objects:   <tf.Variable 'linear/b/packed:0' shape=(192,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_71), but are not present in its tracked objects:   <tf.Variable 'linear/w/packed:0' shape=(192, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_71), but are not present in its tracked objects:   <tf.Variable 'linear/w/packed:0' shape=(192, 96) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_23), but are not present in its tracked objects:   <tf.Variable 'linear/b/packed:0' shape=(96,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_23), but are not present in its tracked objects:   <tf.Variable 'linear/b/packed:0' shape=(96,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"]}],"source":["# Compile model with the TPU strategy\n","with strategy.scope():\n","  model, opt = create_model()\n","  model.compile(optimizer=opt, loss=loss,metrics=[r_square, correlation_coefficient], \n","                  steps_per_execution = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":553,"status":"ok","timestamp":1659521281034,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"},"user_tz":-120},"id":"OJ40MYXqZIbP","outputId":"8a904a76-2e49-4068-ac5c-3ae918b6619a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(128, 110, 4)]      0           []                               \n","                                                                                                  \n"," conv1d (Conv1D)                (128, 110, 192)      11712       ['input_1[0][0]']                \n","                                                                                                  \n"," conv1d_2 (Conv1D)              (128, 110, 192)      15552       ['input_1[0][0]']                \n","                                                                                                  \n"," conv1d_4 (Conv1D)              (128, 110, 192)      19392       ['input_1[0][0]']                \n","                                                                                                  \n"," conv1d_6 (Conv1D)              (128, 110, 192)      23232       ['input_1[0][0]']                \n","                                                                                                  \n"," batch_normalization (BatchNorm  (128, 110, 192)     768         ['conv1d[0][0]']                 \n"," alization)                                                                                       \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (128, 110, 192)     768         ['conv1d_2[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (128, 110, 192)     768         ['conv1d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (128, 110, 192)     768         ['conv1d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation (Activation)        (128, 110, 192)      0           ['batch_normalization[0][0]']    \n","                                                                                                  \n"," activation_2 (Activation)      (128, 110, 192)      0           ['batch_normalization_2[0][0]']  \n","                                                                                                  \n"," activation_4 (Activation)      (128, 110, 192)      0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," activation_6 (Activation)      (128, 110, 192)      0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv1d_1 (Conv1D)              (128, 110, 96)       18528       ['activation[0][0]']             \n","                                                                                                  \n"," conv1d_3 (Conv1D)              (128, 110, 96)       18528       ['activation_2[0][0]']           \n","                                                                                                  \n"," conv1d_5 (Conv1D)              (128, 110, 96)       18528       ['activation_4[0][0]']           \n","                                                                                                  \n"," conv1d_7 (Conv1D)              (128, 110, 96)       18528       ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (128, 110, 96)      384         ['conv1d_1[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (128, 110, 96)      384         ['conv1d_3[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (128, 110, 96)      384         ['conv1d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (128, 110, 96)      384         ['conv1d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_1 (Activation)      (128, 110, 96)       0           ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," activation_3 (Activation)      (128, 110, 96)       0           ['batch_normalization_3[0][0]']  \n","                                                                                                  \n"," activation_5 (Activation)      (128, 110, 96)       0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," activation_7 (Activation)      (128, 110, 96)       0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," add (Add)                      (128, 110, 96)       0           ['activation_1[0][0]',           \n","                                                                  'activation_3[0][0]',           \n","                                                                  'activation_5[0][0]',           \n","                                                                  'activation_7[0][0]']           \n","                                                                                                  \n"," layer_normalization (LayerNorm  (128, 110, 96)      192         ['add[0][0]']                    \n"," alization)                                                                                       \n","                                                                                                  \n"," tf.linalg.matmul (TFOpLambda)  (128, 110, 96)       0           ['layer_normalization[0][0]']    \n","                                                                                                  \n"," tf.reshape (TFOpLambda)        (128, 110, 1, 96)    0           ['tf.linalg.matmul[0][0]']       \n","                                                                                                  \n"," tf.compat.v1.transpose (TFOpLa  (128, 1, 110, 96)   0           ['tf.reshape[0][0]']             \n"," mbda)                                                                                            \n","                                                                                                  \n"," tf.math.multiply (TFOpLambda)  (128, 1, 110, 96)    0           ['tf.compat.v1.transpose[0][0]'] \n","                                                                                                  \n"," tf.__operators__.add_1 (TFOpLa  (128, 1, 110, 96)   0           ['tf.math.multiply[0][0]']       \n"," mbda)                                                                                            \n","                                                                                                  \n"," tf.linalg.matmul_4 (TFOpLambda  (128, 1, 110, 219)  0           ['tf.__operators__.add_1[0][0]'] \n"," )                                                                                                \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (128, 1, 110, 1)    0           ['tf.linalg.matmul_4[0][0]']     \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," tf.zeros_like (TFOpLambda)     (128, 1, 110, 1)     0           ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," tf.concat (TFOpLambda)         (128, 1, 110, 220)   0           ['tf.zeros_like[0][0]',          \n","                                                                  'tf.linalg.matmul_4[0][0]']     \n","                                                                                                  \n"," tf.linalg.matmul_1 (TFOpLambda  (128, 110, 96)      0           ['layer_normalization[0][0]']    \n"," )                                                                                                \n","                                                                                                  \n"," tf.reshape_3 (TFOpLambda)      (128, 1, 220, 110)   0           ['tf.concat[0][0]']              \n","                                                                                                  \n"," tf.reshape_1 (TFOpLambda)      (128, 110, 1, 96)    0           ['tf.linalg.matmul_1[0][0]']     \n","                                                                                                  \n"," tf.slice (TFOpLambda)          (128, 1, 219, 110)   0           ['tf.reshape_3[0][0]']           \n","                                                                                                  \n"," tf.__operators__.add (TFOpLamb  (128, 1, 110, 96)   0           ['tf.math.multiply[0][0]']       \n"," da)                                                                                              \n","                                                                                                  \n"," tf.compat.v1.transpose_1 (TFOp  (128, 1, 110, 96)   0           ['tf.reshape_1[0][0]']           \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.reshape_4 (TFOpLambda)      (128, 1, 110, 219)   0           ['tf.slice[0][0]']               \n","                                                                                                  \n"," tf.linalg.matmul_3 (TFOpLambda  (128, 1, 110, 110)  0           ['tf.__operators__.add[0][0]',   \n"," )                                                                'tf.compat.v1.transpose_1[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," tf.slice_1 (TFOpLambda)        (128, 1, 110, 110)   0           ['tf.reshape_4[0][0]']           \n","                                                                                                  \n"," tf.linalg.matmul_2 (TFOpLambda  (128, 110, 96)      0           ['layer_normalization[0][0]']    \n"," )                                                                                                \n","                                                                                                  \n"," tf.__operators__.add_2 (TFOpLa  (128, 1, 110, 110)  0           ['tf.linalg.matmul_3[0][0]',     \n"," mbda)                                                            'tf.slice_1[0][0]']             \n","                                                                                                  \n"," tf.reshape_2 (TFOpLambda)      (128, 110, 1, 96)    0           ['tf.linalg.matmul_2[0][0]']     \n","                                                                                                  \n"," tf.nn.softmax (TFOpLambda)     (128, 1, 110, 110)   0           ['tf.__operators__.add_2[0][0]'] \n","                                                                                                  \n"," tf.compat.v1.transpose_2 (TFOp  (128, 1, 110, 96)   0           ['tf.reshape_2[0][0]']           \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.linalg.matmul_5 (TFOpLambda  (128, 1, 110, 96)   0           ['tf.nn.softmax[0][0]',          \n"," )                                                                'tf.compat.v1.transpose_2[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," tf.compat.v1.transpose_3 (TFOp  (128, 110, 1, 96)   0           ['tf.linalg.matmul_5[0][0]']     \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.reshape_5 (TFOpLambda)      (128, 110, 96)       0           ['tf.compat.v1.transpose_3[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," tf.linalg.matmul_6 (TFOpLambda  (128, 110, 96)      0           ['tf.reshape_5[0][0]']           \n"," )                                                                                                \n","                                                                                                  \n"," tf.math.add (TFOpLambda)       (128, 110, 96)       0           ['tf.linalg.matmul_6[0][0]']     \n","                                                                                                  \n"," add_1 (Add)                    (128, 110, 96)       0           ['add[0][0]',                    \n","                                                                  'tf.math.add[0][0]']            \n","                                                                                                  \n"," layer_normalization_1 (LayerNo  (128, 110, 96)      192         ['add_1[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," tf.linalg.matmul_7 (TFOpLambda  (128, 110, 192)     0           ['layer_normalization_1[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," tf.math.add_1 (TFOpLambda)     (128, 110, 192)      0           ['tf.linalg.matmul_7[0][0]']     \n","                                                                                                  \n"," activation_8 (Activation)      (128, 110, 192)      0           ['tf.math.add_1[0][0]']          \n","                                                                                                  \n"," tf.linalg.matmul_8 (TFOpLambda  (128, 110, 96)      0           ['activation_8[0][0]']           \n"," )                                                                                                \n","                                                                                                  \n"," tf.math.add_2 (TFOpLambda)     (128, 110, 96)       0           ['tf.linalg.matmul_8[0][0]']     \n","                                                                                                  \n"," add_2 (Add)                    (128, 110, 96)       0           ['add_1[0][0]',                  \n","                                                                  'tf.math.add_2[0][0]']          \n","                                                                                                  \n"," layer_normalization_2 (LayerNo  (128, 110, 96)      192         ['add_2[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," tf.linalg.matmul_9 (TFOpLambda  (128, 110, 96)      0           ['layer_normalization_2[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," tf.reshape_6 (TFOpLambda)      (128, 110, 1, 96)    0           ['tf.linalg.matmul_9[0][0]']     \n","                                                                                                  \n"," tf.compat.v1.transpose_4 (TFOp  (128, 1, 110, 96)   0           ['tf.reshape_6[0][0]']           \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.math.multiply_1 (TFOpLambda  (128, 1, 110, 96)   0           ['tf.compat.v1.transpose_4[0][0]'\n"," )                                                               ]                                \n","                                                                                                  \n"," tf.__operators__.add_4 (TFOpLa  (128, 1, 110, 96)   0           ['tf.math.multiply_1[0][0]']     \n"," mbda)                                                                                            \n","                                                                                                  \n"," tf.linalg.matmul_13 (TFOpLambd  (128, 1, 110, 219)  0           ['tf.__operators__.add_4[0][0]'] \n"," a)                                                                                               \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (128, 1, 110, 1)    0           ['tf.linalg.matmul_13[0][0]']    \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," tf.zeros_like_1 (TFOpLambda)   (128, 1, 110, 1)     0           ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," tf.concat_1 (TFOpLambda)       (128, 1, 110, 220)   0           ['tf.zeros_like_1[0][0]',        \n","                                                                  'tf.linalg.matmul_13[0][0]']    \n","                                                                                                  \n"," tf.linalg.matmul_10 (TFOpLambd  (128, 110, 96)      0           ['layer_normalization_2[0][0]']  \n"," a)                                                                                               \n","                                                                                                  \n"," tf.reshape_9 (TFOpLambda)      (128, 1, 220, 110)   0           ['tf.concat_1[0][0]']            \n","                                                                                                  \n"," tf.reshape_7 (TFOpLambda)      (128, 110, 1, 96)    0           ['tf.linalg.matmul_10[0][0]']    \n","                                                                                                  \n"," tf.slice_2 (TFOpLambda)        (128, 1, 219, 110)   0           ['tf.reshape_9[0][0]']           \n","                                                                                                  \n"," tf.__operators__.add_3 (TFOpLa  (128, 1, 110, 96)   0           ['tf.math.multiply_1[0][0]']     \n"," mbda)                                                                                            \n","                                                                                                  \n"," tf.compat.v1.transpose_5 (TFOp  (128, 1, 110, 96)   0           ['tf.reshape_7[0][0]']           \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.reshape_10 (TFOpLambda)     (128, 1, 110, 219)   0           ['tf.slice_2[0][0]']             \n","                                                                                                  \n"," tf.linalg.matmul_12 (TFOpLambd  (128, 1, 110, 110)  0           ['tf.__operators__.add_3[0][0]', \n"," a)                                                               'tf.compat.v1.transpose_5[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," tf.slice_3 (TFOpLambda)        (128, 1, 110, 110)   0           ['tf.reshape_10[0][0]']          \n","                                                                                                  \n"," tf.linalg.matmul_11 (TFOpLambd  (128, 110, 96)      0           ['layer_normalization_2[0][0]']  \n"," a)                                                                                               \n","                                                                                                  \n"," tf.__operators__.add_5 (TFOpLa  (128, 1, 110, 110)  0           ['tf.linalg.matmul_12[0][0]',    \n"," mbda)                                                            'tf.slice_3[0][0]']             \n","                                                                                                  \n"," tf.reshape_8 (TFOpLambda)      (128, 110, 1, 96)    0           ['tf.linalg.matmul_11[0][0]']    \n","                                                                                                  \n"," tf.nn.softmax_1 (TFOpLambda)   (128, 1, 110, 110)   0           ['tf.__operators__.add_5[0][0]'] \n","                                                                                                  \n"," tf.compat.v1.transpose_6 (TFOp  (128, 1, 110, 96)   0           ['tf.reshape_8[0][0]']           \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.linalg.matmul_14 (TFOpLambd  (128, 1, 110, 96)   0           ['tf.nn.softmax_1[0][0]',        \n"," a)                                                               'tf.compat.v1.transpose_6[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," tf.compat.v1.transpose_7 (TFOp  (128, 110, 1, 96)   0           ['tf.linalg.matmul_14[0][0]']    \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.reshape_11 (TFOpLambda)     (128, 110, 96)       0           ['tf.compat.v1.transpose_7[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," tf.linalg.matmul_15 (TFOpLambd  (128, 110, 96)      0           ['tf.reshape_11[0][0]']          \n"," a)                                                                                               \n","                                                                                                  \n"," tf.math.add_3 (TFOpLambda)     (128, 110, 96)       0           ['tf.linalg.matmul_15[0][0]']    \n","                                                                                                  \n"," add_3 (Add)                    (128, 110, 96)       0           ['add_2[0][0]',                  \n","                                                                  'tf.math.add_3[0][0]']          \n","                                                                                                  \n"," layer_normalization_3 (LayerNo  (128, 110, 96)      192         ['add_3[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," tf.linalg.matmul_16 (TFOpLambd  (128, 110, 192)     0           ['layer_normalization_3[0][0]']  \n"," a)                                                                                               \n","                                                                                                  \n"," tf.math.add_4 (TFOpLambda)     (128, 110, 192)      0           ['tf.linalg.matmul_16[0][0]']    \n","                                                                                                  \n"," activation_9 (Activation)      (128, 110, 192)      0           ['tf.math.add_4[0][0]']          \n","                                                                                                  \n"," tf.linalg.matmul_17 (TFOpLambd  (128, 110, 96)      0           ['activation_9[0][0]']           \n"," a)                                                                                               \n","                                                                                                  \n"," tf.math.add_5 (TFOpLambda)     (128, 110, 96)       0           ['tf.linalg.matmul_17[0][0]']    \n","                                                                                                  \n"," add_4 (Add)                    (128, 110, 96)       0           ['add_3[0][0]',                  \n","                                                                  'tf.math.add_5[0][0]']          \n","                                                                                                  \n"," bidirectional (Bidirectional)  (128, 8)             3232        ['add_4[0][0]']                  \n","                                                                                                  \n"," dense (Dense)                  (128, 1)             9           ['bidirectional[0][0]']          \n","                                                                                                  \n","==================================================================================================\n","Total params: 152,617\n","Trainable params: 150,313\n","Non-trainable params: 2,304\n","__________________________________________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","source":[""],"metadata":{"id":"w_W3xuj25ftG","executionInfo":{"status":"ok","timestamp":1659608110743,"user_tz":-120,"elapsed":482,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["### Model fitting"],"metadata":{"id":"sV3i1j2MDm3A"}},{"cell_type":"code","source":["model.fit(train_dataset.repeat().batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE), \n","          validation_data = validation_dataset.repeat().batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE),\n","          steps_per_epoch = N_train_batches, validation_steps = N_val_batches,\n","          epochs=epochs , callbacks=callbacks_list) \n"," "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1ATKkVh44-1","executionInfo":{"status":"ok","timestamp":1659613983566,"user_tz":-120,"elapsed":4614478,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}},"outputId":"8f17cb59-d137-4b81-af01-805e39d382fa"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"," 2500/36743 [=>............................] - ETA: 11:23 - loss: 2.4339 - r_square: 0.2991 - correlation_coefficient: 0.3204WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 2.9998s vs `on_train_batch_end` time: 6.9826s). Check your callbacks.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 2.9998s vs `on_train_batch_end` time: 6.9826s). Check your callbacks.\n"]},{"output_type":"stream","name":"stdout","text":["36743/36743 [==============================] - ETA: 0s - loss: 0.9097 - r_square: 0.4707 - correlation_coefficient: 0.4870\n","Epoch 1: val_correlation_coefficient improved from -inf to 0.54164, saving model to /content/drive/MyDrive/Colab Notebooks/DREAM/Final/Final_model.h5\n","36743/36743 [==============================] - 484s 13ms/step - loss: 0.9097 - r_square: 0.4707 - correlation_coefficient: 0.4870 - val_loss: 0.5036 - val_r_square: 0.5338 - val_correlation_coefficient: 0.5416\n","Epoch 2/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.4822 - r_square: 0.5395 - correlation_coefficient: 0.5523\n","Epoch 2: val_correlation_coefficient improved from 0.54164 to 0.55608, saving model to /content/drive/MyDrive/Colab Notebooks/DREAM/Final/Final_model.h5\n","36743/36743 [==============================] - 459s 12ms/step - loss: 0.4822 - r_square: 0.5395 - correlation_coefficient: 0.5523 - val_loss: 0.4672 - val_r_square: 0.5463 - val_correlation_coefficient: 0.5561\n","Epoch 3/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.4621 - r_square: 0.5518 - correlation_coefficient: 0.5633\n","Epoch 3: val_correlation_coefficient improved from 0.55608 to 0.56260, saving model to /content/drive/MyDrive/Colab Notebooks/DREAM/Final/Final_model.h5\n","36743/36743 [==============================] - 459s 13ms/step - loss: 0.4621 - r_square: 0.5518 - correlation_coefficient: 0.5633 - val_loss: 0.4628 - val_r_square: 0.5485 - val_correlation_coefficient: 0.5626\n","Epoch 4/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.4553 - r_square: 0.5574 - correlation_coefficient: 0.5683\n","Epoch 4: val_correlation_coefficient improved from 0.56260 to 0.56448, saving model to /content/drive/MyDrive/Colab Notebooks/DREAM/Final/Final_model.h5\n","36743/36743 [==============================] - 459s 12ms/step - loss: 0.4553 - r_square: 0.5574 - correlation_coefficient: 0.5683 - val_loss: 0.4572 - val_r_square: 0.5540 - val_correlation_coefficient: 0.5645\n","Epoch 5/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.4514 - r_square: 0.5609 - correlation_coefficient: 0.5714\n","Epoch 5: val_correlation_coefficient improved from 0.56448 to 0.56471, saving model to /content/drive/MyDrive/Colab Notebooks/DREAM/Final/Final_model.h5\n","36743/36743 [==============================] - 459s 12ms/step - loss: 0.4514 - r_square: 0.5609 - correlation_coefficient: 0.5714 - val_loss: 0.4571 - val_r_square: 0.5536 - val_correlation_coefficient: 0.5647\n","Epoch 6/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.4485 - r_square: 0.5635 - correlation_coefficient: 0.5738\n","Epoch 6: val_correlation_coefficient improved from 0.56471 to 0.56602, saving model to /content/drive/MyDrive/Colab Notebooks/DREAM/Final/Final_model.h5\n","36743/36743 [==============================] - 459s 12ms/step - loss: 0.4485 - r_square: 0.5635 - correlation_coefficient: 0.5738 - val_loss: 0.4555 - val_r_square: 0.5549 - val_correlation_coefficient: 0.5660\n","Epoch 7/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.4464 - r_square: 0.5655 - correlation_coefficient: 0.5755\n","Epoch 7: val_correlation_coefficient improved from 0.56602 to 0.56892, saving model to /content/drive/MyDrive/Colab Notebooks/DREAM/Final/Final_model.h5\n","36743/36743 [==============================] - 460s 13ms/step - loss: 0.4464 - r_square: 0.5655 - correlation_coefficient: 0.5755 - val_loss: 0.4503 - val_r_square: 0.5600 - val_correlation_coefficient: 0.5689\n","Epoch 8/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.4448 - r_square: 0.5670 - correlation_coefficient: 0.5769\n","Epoch 8: val_correlation_coefficient did not improve from 0.56892\n","36743/36743 [==============================] - 454s 12ms/step - loss: 0.4448 - r_square: 0.5670 - correlation_coefficient: 0.5769 - val_loss: 0.4544 - val_r_square: 0.5554 - val_correlation_coefficient: 0.5671\n","Epoch 9/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.4434 - r_square: 0.5682 - correlation_coefficient: 0.5780\n","Epoch 9: val_correlation_coefficient improved from 0.56892 to 0.56969, saving model to /content/drive/MyDrive/Colab Notebooks/DREAM/Final/Final_model.h5\n","36743/36743 [==============================] - 459s 12ms/step - loss: 0.4434 - r_square: 0.5682 - correlation_coefficient: 0.5780 - val_loss: 0.4484 - val_r_square: 0.5619 - val_correlation_coefficient: 0.5697\n","Epoch 10/10\n","36743/36743 [==============================] - ETA: 0s - loss: 0.6206 - r_square: 0.3861 - correlation_coefficient: 0.5757\n","Epoch 10: val_correlation_coefficient did not improve from 0.56969\n","36743/36743 [==============================] - 455s 12ms/step - loss: 0.6206 - r_square: 0.3861 - correlation_coefficient: 0.5757 - val_loss: 5.2899 - val_r_square: -4.4105 - val_correlation_coefficient: 0.1526\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fd051b07490>"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["# Load weights of the best performing model\n","model.load_weights(wd + 'Final_model.h5')\n","\n","# Re-evaluate the model\n","loss, r2, corr = model.evaluate( validation_dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE), \n","               verbose=2)\n","print(\"Model pearson correlation: {:5.2f}\".format(corr))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yfdf8Ud7BfD4","executionInfo":{"status":"ok","timestamp":1659614094132,"user_tz":-120,"elapsed":8655,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}},"outputId":"38dea508-1bee-4688-e508-d2e4c3658e95"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["93/93 - 7s - loss: 0.4488 - r_square: 0.5612 - correlation_coefficient: 0.5692 - 7s/epoch - 78ms/step\n","Model pearson correlation:  0.57\n"]}]},{"cell_type":"markdown","metadata":{"id":"WfL-FXTkX36I"},"source":["## Predictions"]},{"cell_type":"code","source":["test_df = pd.read_csv(wd + 'test_sequences.txt', sep = '\\t', header = None)"],"metadata":{"id":"7LONR64avpTa","executionInfo":{"status":"ok","timestamp":1659614714303,"user_tz":-120,"elapsed":223,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["def old_seq2feature(data):\n","    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n","    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n","    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n","    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n","    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n","\n","    mapper = {'A':A_onehot,'C':C_onehot,'G':G_onehot,'T':T_onehot,'N':N_onehot}\n","\n","    for i in (range(0,len(data))) : \n","        if (len(data[i]) > 110) :\n","            data[i] = data[i][-110:]\n","        elif (len(data[i]) < 110) : \n","            while (len(data[i]) < 110) :\n","                data[i] = 'N'+data[i]\n","\n","    transformed = np.asarray(([[mapper[k] for k in (data[i])] for i in (range(len(data)))]))\n","    return transformed\n","\n","\n","with open(os.path.join(wd + 'test_sequences.txt')) as f:\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    d = list(reader)\n","\n","    \n","\n","sequences = [di[0] for di in d]\n","test_sequences= old_seq2feature(sequences)\n","print(test_sequences.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68EkTkbnEXg2","executionInfo":{"status":"ok","timestamp":1659614478537,"user_tz":-120,"elapsed":3913,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}},"outputId":"329d6625-7ed5-404a-92ec-0803b6f3cc9f"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n"]},{"output_type":"stream","name":"stdout","text":["(71103, 110, 4)\n"]}]},{"cell_type":"code","source":["X = test_sequences\n","batch_size = 1024\n","n_padding = (batch_size*(X.shape[0]//batch_size + 1) - X.shape[0])\n","X_padded = np.concatenate((X,np.repeat(X[0:1,:,:],n_padding,axis=0)))\n","Y_pred_padded = model.predict(X_padded , batch_size = batch_size , verbose=1)\n","Y_pred = Y_pred_padded[:X.shape[0]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2t-YFPet0Hs","executionInfo":{"status":"ok","timestamp":1659614571157,"user_tz":-120,"elapsed":6992,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}},"outputId":"cb6c7a57-30d5-431a-f2a9-bf8c67c01cda"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["70/70 [==============================] - 6s 91ms/step\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"TIvVTwWlt0df","executionInfo":{"status":"ok","timestamp":1659614752060,"user_tz":-120,"elapsed":336,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["test_df[1] = Y_pred"],"metadata":{"id":"aE9D8StVEXwJ","executionInfo":{"status":"ok","timestamp":1659614735900,"user_tz":-120,"elapsed":185,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["test_df.to_csv(wd + 'Predictions.tsv', sep = '\\t', header = None, index=False)"],"metadata":{"id":"yzZpOJU_EYGz","executionInfo":{"status":"ok","timestamp":1659614934398,"user_tz":-120,"elapsed":567,"user":{"displayName":"Andreas Møller","userId":"08004138531387111445"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSGUTVwwQoKs"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Train_test_code.ipynb","provenance":[{"file_id":"1X2DJDLxqfOeleB-IyoJUCjArbcRQEn_F","timestamp":1659509332961},{"file_id":"1mkC5OhefFxW0mU9BmFX0cxnCSPLpAtgE","timestamp":1657697972908}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
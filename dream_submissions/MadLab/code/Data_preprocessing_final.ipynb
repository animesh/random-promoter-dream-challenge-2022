{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"id":"gr7e4vQa5uwq"},"source":["# Preprocessing sequence data\n","\n"],"id":"gr7e4vQa5uwq"},{"cell_type":"markdown","metadata":{"id":"JBaNFeNW5uwt"},"source":["## Libraries"],"id":"JBaNFeNW5uwt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Fj0EM6l5uwt"},"outputs":[],"source":["import pandas as pd\n","import os\n","import csv\n","from tqdm import tqdm\n","import numpy as np\n","import h5py"],"id":"0Fj0EM6l5uwt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"U89dh_PR5uwu"},"outputs":[],"source":["import tensorflow.keras as keras\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.python.client import device_lib\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import  Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten , Conv1D, Concatenate , Permute\n","from tensorflow.keras.layers import Bidirectional,LSTM\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import BatchNormalization, Add , LeakyReLU ,Reshape , Activation , MaxPooling1D , Lambda , Dropout\n","from tensorflow.keras.regularizers import l1_l2\n","from tensorflow.keras.backend import conv1d\n","from tensorflow.python.keras.utils import conv_utils\n","from tensorflow.keras import backend as K"],"id":"U89dh_PR5uwu"},{"cell_type":"markdown","metadata":{"id":"zL535DC25uww"},"source":["## Sequence processing"],"id":"zL535DC25uww"},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcNYkdwb5uwx"},"outputs":[],"source":["#Function for one-hot encoding sequences\n","def seq2feature(data):  # copied from https://github.com/1edv/evolution/blob/master/manuscript_code/model/tpu_model\n","    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n","    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n","    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n","    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n","    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n","\n","    mapper = {'A':A_onehot,'C':C_onehot,'G':G_onehot,'T':T_onehot,'N':N_onehot}\n","    worddim = len(mapper['A'])\n","\n","\n","    transformed = np.asarray(([[mapper[k] for k in (data[i])] for i in (range(len(data)))]))\n","    return transformed"],"id":"rcNYkdwb5uwx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJjLOxSb5uwx","outputId":"efb36143-e06d-4ab4-d622-7c8a55cd8747"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2861270/2861270 [00:01<00:00, 1719771.92it/s]\n"]}],"source":["#Parse sequence file\n","with open(os.path.join('train_sequences.txt')) as f:\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    # Remove sequences with discrete expression value with 50% probability \n","    d = []\n","    for di in reader:\n","      if (float(di[1]) % 1) == 0.:\n","        if np.random.choice([True, False], 1)[0]:\n","          d.append(di)\n","      else:\n","        d.append(di)         \n","sequences = [di[0] for di in d]\n","\n","\n","#Padding with N's is sequences are not 110 bp long\n","for i in tqdm(range(0,len(sequences))) : \n","    if (len(sequences[i]) > 110) :\n","        sequences[i] = sequences[i][-110:]\n","    if (len(sequences[i]) < 110) : \n","        while (len(sequences[i]) < 110) :\n","            sequences[i] = 'N'+sequences[i]\n","            \n"],"id":"uJjLOxSb5uwx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSS21Qtk5uwy"},"outputs":[],"source":["# Apply the one-hot encoding\n","seqdata_transformed = seq2feature(sequences)"],"id":"QSS21Qtk5uwy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkSq0UAn5uwy"},"outputs":[],"source":["# Store sequences in a h5py file\n","with h5py.File('train_onehot_sequences_bool_half.h5', 'w') as hf:\n","    hf.create_dataset(\"onehot_sequences_bool\",  data=seqdata_transformed)\n","print(type(seqdata_transformed[0][0][0]))"],"id":"bkSq0UAn5uwy"},{"cell_type":"code","source":[""],"metadata":{"id":"BTa5DfKD87Dw"},"id":"BTa5DfKD87Dw","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Expression processing"],"metadata":{"id":"L4cPftVx9AVb"},"id":"L4cPftVx9AVb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dfz8ZZXy5uwz"},"outputs":[],"source":["# Store expression values in a h5py file\n","expressions = [di[1] for di in d]\n","expdata = np.asarray(expressions)\n","expdata = expdata.astype('float')  \n","expressions = expdata"],"id":"Dfz8ZZXy5uwz"},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcQqWgxy5uw0"},"outputs":[],"source":["#Make sure no expressioin values are NA before scaling\n","#Copied from https://github.com/1edv/evolution/blob/master/manuscript_code/model/tpu_model/data_processing.ipynb\n","def clean_exp(Y) :\n","    exp_NA = [(a=='NA') for a in Y]\n","    exp_NA = np.array(exp_NA)\n","\n","    Y = np.array(Y)\n","\n","    clean_exp = Y[~exp_NA]\n","    clean_exp = [float(a) for a in clean_exp ]\n","    return clean_exp\n","    \n","clean_trY = np.array(clean_exp(expressions)).reshape(-1, 1)"],"id":"VcQqWgxy5uw0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-ODdUEe5uw0"},"outputs":[],"source":["# Apply and dump scaler\n","from sklearn.preprocessing import StandardScaler\n","from joblib import dump, load\n","scaler = StandardScaler()\n","scaler.fit(clean_trY)\n","expressions = scaler.transform(np.array(expressions).reshape(-1, 1))\n","dump(scaler,'scaler_half.save' ) "],"id":"l-ODdUEe5uw0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSvi0vDk5uw0"},"outputs":[],"source":["# Store scaled expression values in a h5py file\n","with h5py.File('train_expression_half.h5', 'w') as hf:\n","    hf.create_dataset(\"expression\",  data=expressions)  "],"id":"FSvi0vDk5uw0"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Data_preprocessing_final.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}
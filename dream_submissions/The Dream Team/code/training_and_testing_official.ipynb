{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8494b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from default_variables import build_model\n",
    "\n",
    "# Import classes.\n",
    "from default_variables import (\n",
    "    rc_Conv1D,\n",
    "    MultiHeadAttention,\n",
    "    FeedForward,\n",
    "    LayerNormalization\n",
    ")\n",
    "\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from scipy.stats import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import conv1d\n",
    "from tensorflow.keras.layers import (LSTM, Activation, Add, BatchNormalization,\n",
    "                                     Bidirectional, Concatenate, Conv1D,\n",
    "                                     Conv2D, Dense, Dropout, Flatten, Lambda,\n",
    "                                     LeakyReLU, MaxPooling1D, MaxPooling2D,\n",
    "                                     Permute, Reshape, UpSampling2D)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "\n",
    "tf.config.list_logical_devices('TPU')\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(\"local\")\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8aa09",
   "metadata": {},
   "source": [
    "# Required Input (!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training_sequences = \"\"\n",
    "path_to_test_sequences = \"\"\n",
    "\n",
    "output_path_of_final_predictions = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14d555",
   "metadata": {},
   "source": [
    "# Augment the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae876c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_x(list_of_sequences):\n",
    "    def seq2feature(data,mapper,worddim):\n",
    "        transformed = np.zeros([data.shape[0],1,len(data[0]),4] , dtype=np.bool )\n",
    "        for i in tqdm(range(data.shape[0])):\n",
    "            for j,k in enumerate(data[i]):\n",
    "                #print j,k\n",
    "                transformed[i,0,j] = mapper[k] \n",
    "                #print mapper[k]\n",
    "        return transformed\n",
    "\n",
    "    # Add 'N' to sequences that are not of length 110.\n",
    "    for i in range(0,len(list_of_sequences)) : \n",
    "        if (len(list_of_sequences[i]) > 110) :\n",
    "            list_of_sequences[i] = list_of_sequences[i][-110:]\n",
    "        if (len(list_of_sequences[i]) < 110) : \n",
    "            while (len(list_of_sequences[i]) < 110) :\n",
    "                list_of_sequences[i] = 'N'+list_of_sequences[i]\n",
    "                \n",
    "    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n",
    "    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n",
    "    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n",
    "    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n",
    "    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n",
    "    \n",
    "    mapper = {'A':A_onehot,'C':C_onehot,'G':G_onehot,'T':T_onehot,'N':N_onehot}\n",
    "    worddim = len(mapper['A'])\n",
    "    seqdata = np.asarray(list_of_sequences)\n",
    "    \n",
    "    seqdata_transformed = seq2feature(seqdata, mapper, worddim)\n",
    "    \n",
    "    return np.squeeze(seqdata_transformed)\n",
    "\n",
    "def _preprocess_y(list_of_expressions):\n",
    "    return np.asarray(list_of_expressions).astype('float')\n",
    "\n",
    "def _get_augmented_datasets(path_to_train_sequences: str, random_state: int=420, normal_dist_variance: float=0.3):\n",
    "    data = pd.read_csv(\n",
    "        path_to_train_sequences,\n",
    "        delimiter=\"\\t\",\n",
    "        names=[\"sequence\", \"expression\"]\n",
    "    )\n",
    "\n",
    "    number_of_remaining_data_points = data.shape[0] % 1024\n",
    "\n",
    "    data = data.iloc[:data.shape[0] - number_of_remaining_data_points]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[\"sequence\"], \n",
    "        data[\"expression\"],\n",
    "        test_size=(1500)/(data.shape[0] / 1024),\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    validation_train_size = X_train.shape[0] - (1024 * 50)\n",
    "    validation_train_size = validation_train_size/X_train.shape[0]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        train_size=validation_train_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    y_train_df = pd.DataFrame()\n",
    "    y_train_df[\"expression\"] = y_train\n",
    "    \n",
    "    y_train_df[\"new_expression\"] = y_train_df[\"expression\"]\n",
    "    for integer_expression in tqdm(range(18)):\n",
    "        subset = y_train_df[y_train_df[\"expression\"] == integer_expression]\n",
    "        new_expressions = np.random.normal(\n",
    "            integer_expression,\n",
    "            normal_dist_variance,\n",
    "            size=subset.shape[0]\n",
    "        )\n",
    "        y_train_df.loc[subset.index, \"new_expression\"] = new_expressions\n",
    "        \n",
    "    y_train = y_train_df[\"new_expression\"]\n",
    "    \n",
    "    preprocessed_xtrain = _preprocess_x(list(X_train))\n",
    "    print('...done with xtrain')\n",
    "    preprocessed_ytrain = _preprocess_y(list(y_train))\n",
    "    print('...done with ytrain')\n",
    "\n",
    "    preprocessed_xtest = _preprocess_x(list(X_test))\n",
    "    print('...done with xtest')\n",
    "    preprocessed_ytest = _preprocess_y(list(y_test))\n",
    "    print('...done with ytest')\n",
    "\n",
    "    preprocessed_xval = _preprocess_x(list(X_valid))\n",
    "    print('...done with xvalid')\n",
    "    preprocessed_yval = _preprocess_y(list(y_valid))\n",
    "    print('...done with yvalid')\n",
    "\n",
    "    return (\n",
    "        preprocessed_xtrain,\n",
    "        preprocessed_ytrain,\n",
    "        preprocessed_xtest,\n",
    "        preprocessed_ytest,\n",
    "        preprocessed_xval,\n",
    "        preprocessed_yval\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e952394",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_xtrain, preprocessed_ytrain, preprocessed_xtest, preprocessed_ytest, preprocessed_xval, preprocessed_yval = _get_augmented_datasets(path_to_training_sequences, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645152a9",
   "metadata": {},
   "source": [
    "# Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd701da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"n_val_epoch\": 1000,\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"l1_weight\": 0,\n",
    "    \"l2_weight\": 0,\n",
    "    \"motif_conv_hidden\": 256,\n",
    "    \"conv_hidden\": 64,\n",
    "    \"n_hidden\": 64,\n",
    "    \"n_heads\": 8,\n",
    "    \"conv_width_motif\": 30,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"lr\": 0.001,\n",
    "    \"add_cooperativity_layer\": True,\n",
    "    \"n_aux_layers\": 1,\n",
    "    \"n_attention_layers\": 2,\n",
    "    \"attention_dropout_rate\": 0.1,\n",
    "    \"device_type\": \"gpu\",\n",
    "    \"input_shape\": (5151744, 110, 4),\n",
    "    \"loss\": \"mean_squared_error\",\n",
    "    \"optimizer\": \"Nadam\",\n",
    "    \"trainable_layers\": {\n",
    "        \"block1\": {\n",
    "            \"rc_conv1d\": True,\n",
    "            \"conv2d\": True,\n",
    "            \"conv1d\": True\n",
    "        },\n",
    "        \"block2\": {\n",
    "            \"multiheadattention\": True,\n",
    "            \"lstm\": True\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_weights_folder(\n",
    "    output_folder: str=\"~/\",\n",
    "    model_name: str=\"DreamTeam\",\n",
    "    additional_info: str=\"\"\n",
    "):\n",
    "    path_to_model_weights_folder = os.path.join(\n",
    "        output_folder,\n",
    "        model_name,\n",
    "        \n",
    "    )\n",
    "\n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")\n",
    "    checkpoint_path = os.path.join(\n",
    "        path_to_model_weights_folder,\n",
    "        f\"{additional_info}_\" + now\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(path_to_model_weights_folder):\n",
    "        os.mkdir(path_to_model_weights_folder)\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    return checkpoint_path\n",
    "\n",
    "def _create_folders_and_callbacks(\n",
    "    output_folder: str,\n",
    "    model_params: dict,\n",
    "    model_name: str=\"DreamTeam\",\n",
    "    additional_info: str=\"\",\n",
    "):\n",
    "    path_to_weights_folder = _create_weights_folder(\n",
    "        output_folder=output_folder,\n",
    "        model_name=model_name,\n",
    "        additional_info=additional_info\n",
    "    )\n",
    "\n",
    "    path_to_logs = os.path.join(path_to_weights_folder, \"logs\")\n",
    "\n",
    "    if not os.path.exists(path_to_logs):\n",
    "        os.mkdir(path_to_logs)\n",
    "        \n",
    "    folder_of_best_weights = os.path.join(path_to_weights_folder, \"best_weights\")\n",
    "    if not os.path.exists(folder_of_best_weights):\n",
    "        os.mkdir(folder_of_best_weights)\n",
    "\n",
    "    # path_to_weights_folder\n",
    "    #   model_name\n",
    "    #       configuration_file.json\n",
    "    #       logs/       <--- for tensorboard\n",
    "    #           train/\n",
    "    #           validation/\n",
    "    #       best_weights/ <--- best weights according to validation loss\n",
    "    #           best_weights.h5\n",
    "    #       model.epoch-01-loss-0.01-val-loss-0.01.h5 <--- all weights\n",
    "\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        # Save all weights.\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\n",
    "                path_to_weights_folder,\n",
    "                'model.epoch-{epoch:02d}_train-loss-{loss:.5f}_val-loss-{val_loss:.5f}.h5'),\n",
    "            \n",
    "        ),\n",
    "        \n",
    "        # Save best weights.\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\n",
    "                folder_of_best_weights,\n",
    "                'best-weights.h5'\n",
    "            ),\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True\n",
    "        ),\n",
    "\n",
    "        # Tensorboard\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=path_to_logs,\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    \n",
    "    ]\n",
    "\n",
    "    print(f\"Path to weights and config: {path_to_weights_folder}\")\n",
    "    with open(os.path.join(path_to_weights_folder, \"configuration_file.json\"), \"w\") as f:\n",
    "        json.dump(model_params, f)\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1bc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    model = build_model(\n",
    "        model_params=model_params\n",
    "    )\n",
    "    \n",
    "    callbacks = _create_folders_and_callbacks(\n",
    "        output_folder=\"~/\"\n",
    "        model_params=model_params,\n",
    "        model_name=\"DreamTeam_model\",\n",
    "        additional_info=\"\"\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        preprocessed_xtrain,\n",
    "        preprocessed_ytrain,\n",
    "        epochs=model_params[\"epochs\"],\n",
    "        batch_size=model_params[\"batch_size\"],\n",
    "        callbacks = callbacks,\n",
    "        validation_data=(preprocessed_xval, preprocessed_yval)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8aa6c",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c44eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED:\n",
    "path_to_best_weights = \"\" # within the folder \"best_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\n",
    "    path_to_test_sequences, \n",
    "    delimiter=\"\\t\", \n",
    "    header=None, \n",
    "    names=[\"sequence\", \"expression\"]\n",
    ")\n",
    "def _preprocess_x(list_of_sequences):\n",
    "    def seq2feature(data,mapper,worddim):\n",
    "        transformed = np.zeros([data.shape[0],1,len(data[0]),4] , dtype=np.bool )\n",
    "        for i in tqdm(range(data.shape[0])):\n",
    "            for j,k in enumerate(data[i]):\n",
    "                #print j,k\n",
    "                transformed[i,0,j] = mapper[k] \n",
    "                #print mapper[k]\n",
    "        return transformed\n",
    "\n",
    "    # Add 'N' to sequences that are not of length 110.\n",
    "    for i in range(0,len(list_of_sequences)) : \n",
    "        if (len(list_of_sequences[i]) > 110) :\n",
    "            list_of_sequences[i] = list_of_sequences[i][-110:]\n",
    "        if (len(list_of_sequences[i]) < 110) : \n",
    "            while (len(list_of_sequences[i]) < 110) :\n",
    "                list_of_sequences[i] = 'N'+list_of_sequences[i]\n",
    "                \n",
    "    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n",
    "    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n",
    "    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n",
    "    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n",
    "    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n",
    "    \n",
    "    mapper = {'A':A_onehot,'C':C_onehot,'G':G_onehot,'T':T_onehot,'N':N_onehot}\n",
    "    worddim = len(mapper['A'])\n",
    "    seqdata = np.asarray(list_of_sequences)\n",
    "    \n",
    "    seqdata_transformed = seq2feature(seqdata, mapper, worddim)\n",
    "    \n",
    "    return np.squeeze(seqdata_transformed)\n",
    "\n",
    "test_x = _preprocess_x(test_set[\"sequence\"])\n",
    "filled_test_x = np.concatenate([test_x] * 21, axis=0)\n",
    "filled_test_x = np.concatenate([filled_test_x, test_x[:42837, :, :]], axis=0)\n",
    "\n",
    "\n",
    "with tpu_strategy.scope():\n",
    "    model = build_model(\n",
    "        model_params=model_params\n",
    "    )\n",
    "    \n",
    "    model.load_weights(path_to_best_weights)\n",
    "    \n",
    "    pred = model.predict(\n",
    "        filled_test_x, \n",
    "        batch_size=1024, \n",
    "        verbose=1\n",
    "    ) \n",
    "    \n",
    "real_predictions = pred[:71103, :].flatten()\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "with open('/home/b330-admin/data/sample_submission.json', 'r') as f:\n",
    "    ground = json.load(f)\n",
    "\n",
    "indices = np.array([int(indice) for indice in list(ground.keys())])\n",
    "PRED_DATA = OrderedDict()\n",
    "\n",
    "for i in indices:\n",
    "    #Y_pred is an numpy array of dimension (71103,) that contains your\n",
    "    #predictions on the test sequences\n",
    "    PRED_DATA[str(i)] = float(real_predictions[i])\n",
    "    \n",
    "def dump_predictions(prediction_dict, prediction_file):\n",
    "    with open(prediction_file, 'w') as f:\n",
    "        json.dump(prediction_dict, f)\n",
    "    \n",
    "dump_predictions(PRED_DATA, output_path_of_final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

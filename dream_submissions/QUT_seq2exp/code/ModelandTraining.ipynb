{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2e847-b685-4301-a441-2f5058c3864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch \n",
    "import pickle\n",
    "from torch import nn, Tensor \n",
    "import torch, math\n",
    "\n",
    "\n",
    "from utils import PositionalEncoding\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb57e6-478a-43d2-9d31-d9974743d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_length, batch_length, embed_dim\n",
    "\n",
    "embed_dim = 100\n",
    "length = 110\n",
    "\n",
    "t_encoders = 3 # number of Transformer encoder blocks that are stacked\n",
    "\n",
    "# parameter for each block (same for every)\n",
    "encoder_params = {'d_model':embed_dim, # DO NOT CHANGE \n",
    "                  'batch_first': True, # DO NOT CHANGE\n",
    "                  'nhead': 5, # number of heads on multi-attention\n",
    "                  'dropout':0.0, # dropout probability\n",
    "                  'dim_feedforward': 1000 # neurons in hidden feedforward layers\n",
    "                 }\n",
    "\n",
    "PELayer = []\n",
    "\n",
    "PE = True \n",
    " \n",
    "if PE==True: PELayer.append(PositionalEncoding(d_model=embed_dim, max_len=length))\n",
    "t_enc_layers = [t.nn.TransformerEncoderLayer(**encoder_params) \n",
    "                                            for k in range(t_encoders)]\n",
    "\n",
    "#note that we need to flatten and have a final linear layer for the output as the\n",
    "# encoder layers by default output batches of size [seq_len, embed_dim]. This \n",
    "# is simply because that makes it easier to do things for seq2seq models \n",
    "# (standard transformers) \n",
    "\n",
    "myNet = t.nn.Sequential(*PELayer, \n",
    "                        *t_enc_layers,\n",
    "                        t.nn.Flatten(start_dim=1, end_dim=-1), \n",
    "                        t.nn.Linear(length*embed_dim, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db79878-c1d4-4fee-a918-179fd3c6a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' \n",
    "\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import glob\n",
    "\n",
    "class DREAMLazyData(TensorDataset):\n",
    "    def __init__(self, device=device):\n",
    "        super().__init__()\n",
    "        self.path = os.getcwd() + '/jake/pytorch-tensors-all/'\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        zero_pad = (5 - len(str(item)))\n",
    "        \n",
    "        basename = 'Batch' + ('0'*zero_pad) + str(item)\n",
    "        Xfile, yfile = basename + 'X.pt', basename + 'y.pt' \n",
    "        \n",
    "        X, y = t.load(self.path + Xfile).to(device), t.load(self.path + yfile).to(device)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "         return (len(glob.glob1(self.path, \"*.pt\")))//2 - 1000\n",
    "    \n",
    "myData = DREAMLazyData()\n",
    "\n",
    "ldX = DataLoader(myData, batch_size=None, \n",
    "                 shuffle=True, \n",
    "                 generator=torch.Generator(device=device))\n",
    "\n",
    "test_X, test_y = iter(ldX).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e89b4b-e2b7-4c09-b62b-323b580f221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myLoss = t.nn.MSELoss()\n",
    "\n",
    "def evaluate(myModel, start, end):\n",
    "    val_loss = 0\n",
    "    for i in range(start, end): \n",
    "        test_X, test_y = myData.__getitem__(i)\n",
    "\n",
    "        with t.no_grad():  \n",
    "            pred_test = myModel(test_X).reshape(-1)\n",
    "            val_loss += round(myLoss(test_y, pred_test).item(),2)\n",
    "        \n",
    "    return val_loss/(end - start)\n",
    "\n",
    "\n",
    "val_loss =  evaluate(myModel=myNet, start=5750, end=5775)  \n",
    "print(\"Validation Loss:\", round(val_loss,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65531b7e-ed8b-486a-96ef-0b2cbdceb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = t.optim.Adam(params = myNet.parameters())\n",
    "rt = 0\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 5\n",
    "num_iters = 1000 // batch_size\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for k in range(myData.__len__()):\n",
    "        with t.no_grad():\n",
    "            if k % 500 == 0 and (k>0 or i>0): \n",
    "                \n",
    "                val_loss = round(evaluate(myNet,  start=5750, end=5775),2)\n",
    "                print('VALIDATION: ', val_loss)\n",
    "            \n",
    "                filename = os.getcwd() + '/saved_models/'+'DREAMnet_E' \\\n",
    "                                    +str(i+1)+'_F'+ str(k) \\\n",
    "                                    + '__train_' + str(str(round(rt, 2))) \\\n",
    "                                    + '_Val_' + str(val_loss) + '.pt' \n",
    "                \n",
    "                t.save(myNet, filename)\n",
    "        \n",
    "        train_str = ', prev. avg. train: ' + str(round(rt, 2)) if k > 0 else ' '\n",
    "        \n",
    "        print('Epoch', str(i+1) + ',', 'File', str(k+1) + '/' + str(myData.__len__())\n",
    "                 + train_str) \n",
    "        rt = 0\n",
    "        _X, _y = iter(ldX).next()\n",
    "\n",
    "        batchData = TensorDataset(_X, _y)\n",
    "        ldBatch = iter(DataLoader(batchData, batch_size=batch_size,\n",
    "                                generator=torch.Generator(device='cuda'),\n",
    "                                  shuffle=True))\n",
    "\n",
    "        for j in range(len(ldBatch)):\n",
    "            optim.zero_grad() \n",
    "\n",
    "            __X, __y = ldBatch.next() \n",
    "            pred_y = myNet(__X).reshape(-1)  \n",
    "\n",
    "            loss = myLoss(pred_y, __y)\n",
    "            loss.backward()\n",
    "\n",
    "            rt += round(loss.item(),2)/num_iters\n",
    "            optim.step()\n",
    "            \n",
    "                     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

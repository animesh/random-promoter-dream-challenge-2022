{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    list_x = []\n",
    "    list_y = []\n",
    "    ins = open(filename, \"r\")\n",
    "    for line in ins:\n",
    "        cache = line.rstrip('\\n')\n",
    "        cache = cache.replace(\"A\", \"1\")\n",
    "        cache = cache.replace(\"C\", \"2\")\n",
    "        cache = cache.replace(\"G\", \"3\")\n",
    "        cache = cache.replace(\"T\", \"4\")\n",
    "        cache = cache.replace(\"N\", \"5\")\n",
    "        list_x.append(cache.split('\\t')[0])\n",
    "        list_y.append(cache.split('\\t')[1])\n",
    "    x = np.array(list_x)\n",
    "    y = np.array(list_y)\n",
    "    x = x.reshape(len(x), 1)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def read_file_105_110(filename):  # 只读取105_110长度的序列存入矩阵\n",
    "    list_x = []\n",
    "    list_y = []\n",
    "    ins = open(filename, \"r\")\n",
    "    for line in ins:\n",
    "        cache = line.rstrip('\\n')\n",
    "        cache = cache.replace(\"A\", \"1\")\n",
    "        cache = cache.replace(\"C\", \"2\")\n",
    "        cache = cache.replace(\"G\", \"3\")\n",
    "        cache = cache.replace(\"T\", \"4\")\n",
    "        cache = cache.replace(\"N\", \"5\")\n",
    "        if 104 < len(cache.split('\\t')[0]) < 111:\n",
    "            list_x.append(cache.split('\\t')[0])\n",
    "            list_y.append(cache.split('\\t')[1])\n",
    "    x = np.array(list_x)\n",
    "    y = np.array(list_y)\n",
    "    x = x.reshape(len(x), 1)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def read_file_105_110_withoutN(filename):  # 只读取105_110长度的序列存入矩阵,有N就去掉\n",
    "    list_x = []\n",
    "    list_y = []\n",
    "    ins = open(filename, \"r\")\n",
    "    for line in ins:\n",
    "        cache = line.rstrip('\\n')\n",
    "        cache = cache.replace(\"A\", \"1\")\n",
    "        cache = cache.replace(\"C\", \"2\")\n",
    "        cache = cache.replace(\"G\", \"3\")\n",
    "        cache = cache.replace(\"T\", \"4\")\n",
    "        if 104 < len(cache.split('\\t')[0]) < 111:\n",
    "            if not (\"N\" in cache.split('\\t')[0]):\n",
    "                list_x.append(cache.split('\\t')[0])\n",
    "                list_y.append(cache.split('\\t')[1])\n",
    "    x = np.array(list_x)\n",
    "    y = np.array(list_y)\n",
    "    x = x.reshape(len(x), 1)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    return x, y\n",
    "\n",
    "def read_file_110_withoutN(filename):  # 只读取105_110长度的序列存入矩阵,有N就去掉\n",
    "    list_x = []\n",
    "    list_y = []\n",
    "    cache_x = []\n",
    "    ins = open(filename, \"r\")\n",
    "    for line in ins:\n",
    "        cache = line.rstrip('\\n')\n",
    "        cache = cache.replace(\"A\", \"1\")\n",
    "        cache = cache.replace(\"C\", \"2\")\n",
    "        cache = cache.replace(\"G\", \"3\")\n",
    "        cache = cache.replace(\"T\", \"4\")\n",
    "        if len(cache.split('\\t')[0]) ==110:\n",
    "            if not (\"N\" in cache.split('\\t')[0]):\n",
    "                cache_x.append(cache.split('\\t')[1])\n",
    "                a=cache_x[len(cache_x)-1]\n",
    "                if (float(a)==int(float(a))):\n",
    "                    list_x.append(cache.split('\\t')[0])\n",
    "                    list_y.append(cache.split('\\t')[1])\n",
    "    x = np.array(list_x)\n",
    "    y = np.array(list_y)\n",
    "    x = x.reshape(len(x), 1)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    return x, y\n",
    "\n",
    "def read_file_test(filename):  # 只读取105_110长度的序列存入矩阵,有N就去掉\n",
    "    list_x = []\n",
    "    list_y = []\n",
    "    seq = []\n",
    "    ins = open(filename, \"r\")\n",
    "    for line in ins:\n",
    "        cache = line.rstrip('\\n')\n",
    "        seq.append(cache.split('\\t')[0])\n",
    "        cache = cache.replace(\"A\", \"1\")\n",
    "        cache = cache.replace(\"C\", \"2\")\n",
    "        cache = cache.replace(\"G\", \"3\")\n",
    "        cache = cache.replace(\"T\", \"4\")\n",
    "        list_x.append(cache.split('\\t')[0])\n",
    "        list_y.append(cache.split('\\t')[1])\n",
    "    x = np.array(list_x)\n",
    "    y = np.array(list_y)\n",
    "    x = x.reshape(len(x), 1)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    return x, y,seq\n",
    "\n",
    "def max_len_matrix(matrix):  # 矩阵大小应为n*1，返回最长序列的长度\n",
    "    num_max = 0\n",
    "    for i in range(len(matrix)):\n",
    "        a = matrix[i, 0]\n",
    "        if len(a) > num_max:\n",
    "            num_max = len(a)\n",
    "    return num_max\n",
    "\n",
    "\n",
    "# num_max,num=max_len_matrix(train_x)\n",
    "# out:142，大于110序列的有41136个\n",
    "# print(num_max,num)\n",
    "# print(train_x.shape)\n",
    "def info_len_matrix(matrix):  # 矩阵大小应为n*1，返回序列的长度信息\n",
    "    num_min = 143\n",
    "    num_110 = 0\n",
    "    num_105 = 0\n",
    "    for i in range(len(matrix)):\n",
    "        a = matrix[i, 0]\n",
    "        if len(a) < 110:\n",
    "            num_110 = num_110 + 1\n",
    "        if len(a) < 105:\n",
    "            num_105 = num_105 + 1\n",
    "        if len(a) < num_min:\n",
    "            num_min = len(a)\n",
    "    print(\"最小长度\", num_min)  # output:78\n",
    "    print(\"小于110序列长度的有\", num_110)  # output:1168938 小于108:152750 小于109:319525\n",
    "    print(\"小于105序列长度的有\", num_105)  # 小于105:62448 小于104:51072 小于100:26142\n",
    "    return num_min\n",
    "\n",
    "\n",
    "def padding_x(matrix, num_max):  # 矩阵大小应为n*1,num_max为现有数据最大长度，将长度小于最长序列的序列补0到最长序列\n",
    "    for i in range(len(matrix)):\n",
    "        a = matrix[i, 0]\n",
    "        if len(a) < num_max:\n",
    "            for j in range(num_max - len(a)):\n",
    "                matrix[i, 0] = matrix[i, 0] + \"0\"\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def expend_matrix(matrix):\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[0]) - 1):\n",
    "            matrix[i][j + 1] = matrix[i, 0][j]\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def process_data_max142(filename):\n",
    "    x, y = read_file(filename)\n",
    "    max_num = max_len_matrix(x)\n",
    "    x = padding_x(x, 142)  # 现在序列长度全都是142,不到142的用0扩充到142\n",
    "\n",
    "    # 现在有六百万条数据，每个数据有142个序列。现在将每个数据的序列展开，方便以后调用。shape为6739258, 143\n",
    "    # 本来一条序列有142，多出来的一是原来的数据是原来的序列：6739258，0.\n",
    "    x = np.hstack((x, np.ones((len(x), max_num))))  # 扩展矩阵\n",
    "\n",
    "    x = expend_matrix(x)  # 向扩展后的矩阵写入数据，即将每一条序列展开\n",
    "    x = x.astype(np.float64)  # 转化成数字方便计算\n",
    "    y = y.astype(np.float64)  # 转化成数字方便计算\n",
    "    x = np.delete(x, 0, axis=1)  # 删掉原来的长序列，现在矩阵中一行就是一个序列，一行有142个列\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def process_data_105_110(filename):  # 序列长度只要105_110\n",
    "    x, y = read_file_105_110(filename)\n",
    "    x = padding_x(x, 110)  # 现在序列长度全都是110,不到110的用0扩充到110\n",
    "    x = np.hstack((x, np.ones((len(x), 110))))  # 扩展矩阵\n",
    "    x = expend_matrix(x)  # 向扩展后的矩阵写入数据，即将每一条序列展开\n",
    "    x = x.astype(np.float64)  # 转化成数字方便计算\n",
    "    y = y.astype(np.float64)  # 转化成数字方便计算\n",
    "    x = np.delete(x, 0, axis=1)  # 删掉原来的长序列，现在矩阵中一行就是一个序列，一行有142个列\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def process_data_110_withoutN(filename):  # 序列长度只要105_110没有N\n",
    "    # 因为有N的序列 最小长度 78\n",
    "    # 小于110序列长度的有 5745，即有N的序列只有五千条\n",
    "    # 小于105序列长度的有 188\n",
    "    # 所以决定先去掉N\n",
    "    x, y = read_file_110_withoutN(filename)\n",
    "    x = padding_x(x, 110)  # 现在序列长度全都是110,不到110的用0扩充到110\n",
    "    x = np.hstack((x, np.ones((len(x), 110))))  # 扩展矩阵\n",
    "    x = expend_matrix(x)  # 向扩展后的矩阵写入数据，即将每一条序列展开\n",
    "    x = x.astype(np.float64)  # 转化成数字方便计算\n",
    "    y = y.astype(np.float64)  # 转化成数字方便计算\n",
    "    x = np.delete(x, 0, axis=1)  # 删掉原来的长序列，现在矩阵中一行就是一个序列，一行有142个列\n",
    "    return x, y\n",
    "\n",
    "def process_data_test(filename):  # 序列长度只要105_110没有N\n",
    "    # 因为有N的序列 最小长度 78\n",
    "    # 小于110序列长度的有 5745，即有N的序列只有五千条\n",
    "    # 小于105序列长度的有 188\n",
    "    # 所以决定先去掉N\n",
    "    x, y,seq = read_file_test(filename)\n",
    "    x = np.hstack((x, np.ones((len(x), 110))))  # 扩展矩阵\n",
    "    x = expend_matrix(x)  # 向扩展后的矩阵写入数据，即将每一条序列展开\n",
    "    x = x.astype(np.float64)  # 转化成数字方便计算\n",
    "    y = y.astype(np.float64)  # 转化成数字方便计算\n",
    "    x = np.delete(x, 0, axis=1)  # 删掉原来的长序列，现在矩阵中一行就是一个序列，一行有142个列\n",
    "    return x,y,seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "\n",
    "    def __init__(self, num_hiddens, dropout, max_len=110):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 创建一个足够长的P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        x = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(x)\n",
    "        self.P[:, :, 1::2] = torch.cos(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.P[:, :x.shape[1], :].to(device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_hiddens, sequence_type, max_len, multi_hiddens, num_heads,\n",
    "                 num_layers, ffn_num_input, ffn_num_hiddens, norm1, norm2, dropout,number_of_categories, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding(sequence_type, num_hiddens) # embedding\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout, max_len)  # 位置编码\n",
    "\n",
    "        self.blks = nn.Sequential()  # 定义数个encoder\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\" + str(i),\n",
    "                                 EncoderBlock(num_hiddens, multi_hiddens, num_heads,\n",
    "                                              ffn_num_input, ffn_num_hiddens, norm1, norm2, dropout))\n",
    "        '''测试encoder\n",
    "        self.encoder = EncoderBlock(num_hiddens,multi_hiddens,num_heads,\n",
    "                 ffn_num_input,ffn_num_hiddens,norm1,norm2,dropout)\n",
    "        '''\n",
    "        self.all_in_one = nn.Linear(max_len * num_hiddens, number_of_categories, bias=True)  # 用bias\n",
    "\n",
    "    def forward(self, train_x):\n",
    "#         train_x = self.pos_encoding(self.embedding(train_x) * torch.sqrt(torch.Tensor(self.num_hiddens)).cuda())  # 因为位置编码值在-1和1之间，\n",
    "        train_x = self.pos_encoding(self.embedding(train_x) * math.sqrt(self.num_hiddens))\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放，\n",
    "        # 然后再与位置编码相加。\n",
    "        # 以上完成了位置编码\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            train_x = blk(train_x)\n",
    "            self.attention_weights[\n",
    "                i] = blk.attention.attention.attention_weights\n",
    "        '''\n",
    "        train_x = self.encoder(train_x)\n",
    "        '''\n",
    "        # 把（batch，序列长度，信息维度）直接降维到（batch，1） 用全连接层，或者是全局平均池化\n",
    "        # train_x = torch.nn.functional.adaptive_avg_pool2d(train_x, (1,1)) #全局平均池化\n",
    "        # train_x = train_x.reshape(-1,1)\n",
    "        \"\"\"全连接层\"\"\"\n",
    "        train_x = train_x.reshape(-1, self.max_len * self.num_hiddens)\n",
    "        train_x = self.all_in_one(train_x)\n",
    "        return train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # queries的形状：(batch_size，查询的个数，d)\n",
    "    # keys的形状：(batch_size，“键－值”对的个数，d)\n",
    "    # values的形状：(batch_size，“键－值”对的个数，值的维度)\n",
    "    def forward(self, queries, keys, values):\n",
    "        d = queries.shape[-1]\n",
    "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=True, **kwargs):  # 没有用bias\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # queries，keys，values的形状:\n",
    "        # (batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        # 经过变换后，输出的queries，keys，values　的形状:\n",
    "        # (batch_size*num_heads，查询或者“键－值”对的个数，\n",
    "        # num_hiddens/num_heads)\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        # output的形状:(batch_size*num_heads，查询的个数，\n",
    "        # num_hiddens/num_heads)\n",
    "        output = self.attention(queries, keys, values)\n",
    "\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "\n",
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，\n",
    "    # num_hiddens/num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "\n",
    "class PositionWiseFFN(nn.Module):  # 前馈网络完成\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
    "                 **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "\n",
    "# 归一化完成\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, multi_hiddens, num_heads,\n",
    "                 ffn_num_input, ffn_num_hiddens, norm1, norm2, dropout, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,  # 定义多头注意力\n",
    "                                            multi_hiddens, num_heads, dropout)\n",
    "\n",
    "        self.addnorm1 = AddNorm(norm1, dropout)  # 定义归一化层 （110，16）\n",
    "\n",
    "        self.ffn = PositionWiseFFN(  # 定义前馈网络\n",
    "            ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "\n",
    "        self.addnorm2 = AddNorm(norm2, dropout)  # 定义归一化层 （110，16）\n",
    "\n",
    "    def forward(self, train_x):\n",
    "        cache_1 = self.addnorm1(train_x, self.attention(train_x, train_x, train_x))\n",
    "        train_x = self.addnorm2(cache_1, self.ffn(cache_1))\n",
    "        return train_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(data.Dataset):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_x[idx], self.train_y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_f1(x,y,number_of_categories):\n",
    "    \"\"\"测试设置\"\"\"\n",
    "    with torch.no_grad(): \n",
    "        \n",
    "        net.eval()\n",
    "        \n",
    "        \"\"\"与net.train对应\"\"\"\n",
    "        tp=fn=fp=tn= 0\n",
    "        #定义一个表，（actual，predicted）\n",
    "        table=[[0 for i in range(number_of_categories)]for i in range(number_of_categories)]\n",
    "        true = 0\n",
    "        false = 0\n",
    "        for length in range(x.shape[0]):\n",
    "            x_hat = net(x[length].reshape(1,110))\n",
    "            x_hat = x_hat.reshape(number_of_categories)\n",
    "            table[y[length, 0]][torch.argmax(x_hat, dim=0)] +=1\n",
    "        #         x_hat = x_hat.detach().cpu().numpy()\n",
    "            if torch.argmax(x_hat, dim=0) == y[length, 0]:\n",
    "                true += 1\n",
    "            #计算tpfnfptn,F1-score微平均(micro-average)\n",
    "        table = np.array(table)\n",
    "        fp = table.sum(axis=0) - np.diag(table)\n",
    "        fp = fp.sum()\n",
    "        fn = table.sum(axis=1) - np.diag(table)\n",
    "        fn = fn.sum()\n",
    "        tp = np.diag(table)\n",
    "        tp = tp.sum()\n",
    "        tn = table.sum()*number_of_categories - (fp + fn + tp)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1_out=(2*precision*recall)/(precision+recall)\n",
    "        acc = true / x.shape[0]\n",
    "    return acc,f1_out\n",
    "\n",
    "def acc_epoch_f1(x,y,epoch,number_of_categories):\n",
    "    \"\"\"测试设置\"\"\"\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        net.eval()\n",
    "        \n",
    "        \"\"\"与net.train对应\"\"\"\n",
    "        tp=fn=fp=tn= 0\n",
    "        #定义一个表，（actual，predicted）\n",
    "        table=[[0 for i in range(number_of_categories)]for i in range(number_of_categories)]\n",
    "        true = 0\n",
    "        false = 0\n",
    "        loss_sum = 0\n",
    "        for length in range(x.shape[0]):\n",
    "            x_hat = net(x[length].reshape(1,110))\n",
    "            y_hat=y[length].reshape(1,1)\n",
    "            loss_sum += criterion(x_hat,y_hat.view(-1)).item()\n",
    "            x_hat = x_hat.reshape(number_of_categories)\n",
    "        #         x_hat = x_hat.detach().cpu().numpy()\n",
    "            table[y[length, 0]][torch.argmax(x_hat, dim=0)] +=1\n",
    "            if torch.argmax(x_hat, dim=0) == y[length, 0]:\n",
    "                true += 1\n",
    "        cache = 'Epoch: {} and loss ={}'.format(epoch+1,loss_sum)\n",
    "        write_file(output_file_val_loss,cache)\n",
    "        #计算tpfnfptn,F1-score微平均(micro-average)\n",
    "        table = np.array(table)\n",
    "        fp = table.sum(axis=0) - np.diag(table)\n",
    "        fp = fp.sum()\n",
    "        fn = table.sum(axis=1) - np.diag(table)\n",
    "        fn = fn.sum()\n",
    "        tp = np.diag(table)\n",
    "        tp = tp.sum()\n",
    "        tn = table.sum()*number_of_categories - (fp + fn + tp)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1_out=(2*precision*recall)/(precision+recall)\n",
    "        acc = true / x.shape[0]\n",
    "    return acc,f1_out\n",
    "\n",
    "\n",
    "def f1(x,y,number_of_categories):\n",
    "    \"\"\"测试设置\"\"\"\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        net.eval()\n",
    "        \"\"\"与net.train对应\"\"\"\n",
    "\n",
    "        tp=fn=fp=tn= 0\n",
    "    \n",
    "    #定义一个表，（actual，predicted）\n",
    "        table=[[0 for i in range(number_of_categories)]for i in range(number_of_categories)]\n",
    "    \n",
    "        for length in range(x.shape[0]):\n",
    "            x_hat = net(x[length].reshape(1,110))\n",
    "            x_hat = x_hat.reshape(number_of_categories)\n",
    "            table[y[length, 0]][torch.argmax(x_hat, dim=0)] +=1\n",
    "    \n",
    "    #计算tpfnfptn,F1-score微平均(micro-average)\n",
    "        table = np.array(table)\n",
    "        fp = table.sum(axis=0) - np.diag(table)\n",
    "        fp = fp.sum()\n",
    "        fn = table.sum(axis=1) - np.diag(table)\n",
    "        fn = fn.sum()\n",
    "        tp = np.diag(table)\n",
    "        tp = tp.sum()\n",
    "        tn = table.sum()*number_of_categories - (fp + fn + tp)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1_out=(2*precision*recall)/(precision+recall)\n",
    "    return f1_out\n",
    "\n",
    "def f1_2(x,y,number_of_categories):\n",
    "    \"\"\"测试设置\"\"\"\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        net.eval()\n",
    "        \"\"\"与net.train对应\"\"\"\n",
    "\n",
    "        tp=fn=fp=tn= 0\n",
    "    \n",
    "    #定义一个表，（actual，predicted）\n",
    "        table=[[0 for i in range(number_of_categories)]for i in range(number_of_categories)]\n",
    "    \n",
    "        for length in range(x.shape[0]):\n",
    "            x_hat = net(x[length].reshape(1,110))\n",
    "            x_hat = x_hat.reshape(number_of_categories)\n",
    "            table[y[length, 0]][torch.argmax(x_hat, dim=0)] +=1\n",
    "    \n",
    "    #计算tpfnfptn,F1-score微平均(micro-average)\n",
    "        table = np.array(table)\n",
    "        fp = table[1][0]\n",
    "        fn = table[0][1]\n",
    "        tp = table[0][0]\n",
    "        tn = table[1][1]\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1_out=(2*precision*recall)/(precision+recall)\n",
    "    return f1_out\n",
    "\n",
    "def write_file(filename,file_input):\n",
    "    file1 = open(filename, 'a')\n",
    "    file1.write(file_input+'\\n')\n",
    "    file1.close()\n",
    "    return 0\n",
    "def from18to5(y):\n",
    "    list_y = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i]<=9:\n",
    "            list_y.append('0')\n",
    "        if 9<y[i]<=10:\n",
    "            list_y.append('1')\n",
    "        if 10<y[i]<=11:\n",
    "            list_y.append('2')\n",
    "        if 11<y[i]<=13:\n",
    "            list_y.append('3')\n",
    "        if y[i]>13:\n",
    "            list_y.append('4')\n",
    "    out_y = np.array(list_y)\n",
    "    out_y = out_y.reshape(len(out_y), 1)\n",
    "    out_y = out_y.astype(np.float64)\n",
    "    return out_y\n",
    "def from18to2(y):\n",
    "    list_y = [] \n",
    "    for i in range(len(y)):\n",
    "        if y[i]<=11:\n",
    "            list_y.append('0')\n",
    "        if y[i]>11:\n",
    "            list_y.append('1')\n",
    "    out_y = np.array(list_y)\n",
    "    out_y = out_y.reshape(len(out_y), 1)\n",
    "    out_y = out_y.astype(np.float64)\n",
    "    return out_y\n",
    "def predict(test_x,seq,out_file_name,out_file_weight):\n",
    "    torch.save(net.state_dict(), out_file_weight)\n",
    "    \"\"\"测试设置\"\"\"\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        net.eval()\n",
    "        \n",
    "        \"\"\"与net.train对应\"\"\"\n",
    "        y=np.zeros([test_x.shape[0],3])\n",
    "        cache_y = []\n",
    "        file = open(out_file_name, 'a')\n",
    "        for length in range(test_x.shape[0]):\n",
    "            x_hat = net(test_x[length].reshape(1,110))\n",
    "            x_hat = x_hat.reshape(number_of_categories)\n",
    "            y[length, 1] = torch.argmax(x_hat, dim=0)\n",
    "            y[length, 2] = x_hat[torch.argmax(x_hat, dim=0)]\n",
    "        for i in range(number_of_categories):\n",
    "            max = 0\n",
    "            for len_y in range(y.shape[0]):\n",
    "                if y[len_y, 1] == i:\n",
    "                    if y[len_y, 2] > max:\n",
    "                        max = y[len_y, 2]\n",
    "            cache_y.append(max)\n",
    "        for leng in range(test_x.shape[0]):\n",
    "            y[leng, 0] = y[leng, 1] + y[leng, 2]/10**(len(str(int(cache_y[int(y[leng, 1])]))))\n",
    "            file.write(seq[leng]+'\\t')\n",
    "            file.write(str(y[leng, 0])+'\\n')\n",
    "        file.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 载入文件数据并处理为矩阵形式\n",
    "# train_sequences = \"../data/all_long_train.txt\"\n",
    "# test_sequences = \"../data/all_long_have10000.txt\"\n",
    "# vaildation_sequences = \"../data/all_long_no10000.txt\"\n",
    "\n",
    "# test_sequences = \"../data/all_long_110_test.txt\"\n",
    "\n",
    "train_sequences = \"../data/all_long_110_train.txt\"\n",
    "test_sequences = \"../data/test_sequences.txt\"\n",
    "vaildation_sequences = \"../data/all_long_110_val.txt\"\n",
    "\n",
    "\n",
    "\"\"\"载入训练集数据\"\"\"\n",
    "train_x_init, train_y_init = process_data_110_withoutN(train_sequences)\n",
    "# 训练集大小为(序列的数量，序列的长度)\n",
    "# 由于训练集序列长度也是110，只有四万个序列超过了110.本次训练将超过110小于105的长度抛弃。\n",
    "\"\"\"载入验证集数据\"\"\"\n",
    "vaildation_x_init, vaildation_y_init = process_data_110_withoutN(vaildation_sequences)\n",
    "\"\"\"载入测试集数据\"\"\"\n",
    "test_x_init, test_y_init, seq = process_data_test(test_sequences)\n",
    "\n",
    "#减少箱子数量\n",
    "# train_y_init = from18to2(train_y_init)\n",
    "# test_y_init = from18to2(test_y_init)\n",
    "# vaildation_y_init = from18to2(vaildation_y_init)\n",
    "\n",
    "\n",
    "# 处理数据\n",
    "train_x_init = torch.from_numpy(train_x_init).to(device)  # 数据转换到tensor\n",
    "train_y_init = torch.from_numpy(train_y_init).to(device)\n",
    "\n",
    "test_x_init = torch.from_numpy(test_x_init).to(device)   # 数据转换到tensor\n",
    "test_y_init = torch.from_numpy(test_y_init).to(device)\n",
    "\n",
    "vaildation_x_init = torch.from_numpy(vaildation_x_init).to(device)\n",
    "vaildation_y_init = torch.from_numpy(vaildation_y_init).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     60\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 61\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m cache1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and loss =\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,loss_sum)\n\u001b[1;32m     64\u001b[0m write_file(output_file_train_loss,cache1)\n",
      "File \u001b[0;32m~/.conda/envs/pctn/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pctn/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pctn/lib/python3.8/site-packages/torch/optim/sgd.py:110\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m             momentum_buffer_list\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 110\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m      \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/.conda/envs/pctn/lib/python3.8/site-packages/torch/optim/_functional.py:173\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    171\u001b[0m     momentum_buffer_list[i] \u001b[38;5;241m=\u001b[39m buf\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdampening\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov:\n\u001b[1;32m    176\u001b[0m     d_p \u001b[38;5;241m=\u001b[39m d_p\u001b[38;5;241m.\u001b[39madd(buf, alpha\u001b[38;5;241m=\u001b[39mmomentum)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#数据转换\n",
    "train_x = train_x_init.long()  # 数据转换到long\n",
    "train_y = train_y_init.long()  # 数据转换到long 只保留了整数,实现18个箱子，后期可以根据精准度分为180，1800个箱子\n",
    "\n",
    "test_x = test_x_init.long() # 数据转换到long\n",
    "test_y = test_y_init.long()  # 数据转换到long 只保留了整数,实现18个箱子，后期可以根据精准度分为180，1800个箱子\n",
    "\n",
    "vaildation_x = vaildation_x_init.long() # 数据转换到long\n",
    "vaildation_y = vaildation_y_init.long()\n",
    "\n",
    "sequence_type = 5  # 序列词汇种类\n",
    "num_hiddens = 32  # 字 Embedding 的维度32\n",
    "max_len = 110  # 序列最长长度\n",
    "dropout = 0.1  # 设置dropout为0.1\n",
    "multi_hiddens = 32  # 多头注意力的隐藏层，也是输出维度32\n",
    "num_heads = 8  # 多头注意力设置4个头\n",
    "ffn_num_input = 32  # 前馈网络第三维度32\n",
    "ffn_num_hiddens = 64  # 隐藏层64\n",
    "norm1 = [110, 32]  # 第一个归一化维度32\n",
    "norm2 = [110, 32]  # 第二个归一化维度32\n",
    "num_layers = 2  # 定义encoder的数量\n",
    "batch_size = 2048  # 一次训练样本数量\n",
    "number_of_categories = 18\n",
    "\n",
    "output_file_train_loss = './project1_train_loss.txt'\n",
    "output_file_val_loss = './project1_val_loss.txt'\n",
    "output_file_vaildation_acc = './project1_vaildation_acc.txt'\n",
    "output_file_test_acc = './project1_test_acc.txt'\n",
    "output_file_time = './project1_time.txt'\n",
    "output_file_vaildation_f1 = './project1_vaildation_f1.txt'\n",
    "output_file_test_f1 = './project1_test_f1.txt'\n",
    "output_file_weight = 'project1_weight.pth'\n",
    "output_file_test = 'project1_test.txt'\n",
    "\n",
    "\n",
    "\n",
    "# 上测试集查看准确率\n",
    "\n",
    "loader = data.DataLoader(MyDataSet(train_x, train_y), batch_size, shuffle=True)\n",
    "#   num_workers=8,第四个参数 ： 多线程读数据\n",
    "\n",
    "# 定义网络\n",
    "net = Net(num_hiddens, sequence_type, max_len, multi_hiddens, num_heads,\n",
    "          num_layers, ffn_num_input, ffn_num_hiddens, norm1, norm2, dropout,number_of_categories).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "# 开始训练\n",
    "for epoch in range(1000000):\n",
    "    start = time.time()\n",
    "    \"\"\"与acc测试对应\"\"\"\n",
    "    net.train()\n",
    "    \"\"\"以上\"\"\"\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for train_x, train_y in loader:\n",
    "        train_x = net(train_x)\n",
    "        loss = criterion(train_x, train_y.view(-1))\n",
    "        loss_sum+=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    cache1 = 'Epoch: {} and loss ={}'.format(epoch+1,loss_sum)\n",
    "    write_file(output_file_train_loss,cache1)\n",
    "    \n",
    "    vaildation_acc,vaildation__fl = acc_epoch_f1(vaildation_x,vaildation_y,epoch,number_of_categories)\n",
    "#     test_acc,test__fl = acc_f1(test_x,test_y,number_of_categories)\n",
    "    \n",
    "    cache2=\"Epoch: {} and vaildation_acc: {}\".format(epoch + 1,vaildation_acc)\n",
    "    write_file(output_file_vaildation_acc,cache2)\n",
    "#     cache3=\"Epoch: {} and test_acc is is: {}\".format(epoch + 1,test_acc)\n",
    "#     write_file(output_file_test_acc,cache3)\n",
    "    cache4=\"Epoch: {} and vaildation__fl: {}\".format(epoch + 1,vaildation__fl)\n",
    "    write_file(output_file_vaildation_f1,cache4)\n",
    "#     cache5=\"Epoch: {} and test__fl is is: {}\".format(epoch + 1,test__fl)\n",
    "#     write_file(output_file_test_f1,cache5)\n",
    "    if (epoch % 40 == 0):\n",
    "        epo=str(epoch)\n",
    "        predict(test_x,seq,epo+output_file_test,epo+output_file_weight)\n",
    "    end = time.time()\n",
    "    cache='epoch {} took {} seconds'.format(epoch +1, end - start)\n",
    "    write_file(output_file_time,cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
